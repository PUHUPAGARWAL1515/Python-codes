{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6C5WybjB4Fz5d5AAx3XBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/Clustering_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW95KlF7FV7x"
      },
      "outputs": [],
      "source": [
        "#1.What is unsupervised learning in the context of machine learning?\n",
        "\n",
        "Unsupervised learning is a type of machine learning where algorithms learn patterns from data that has not been labeled or categorized. Instead of predicting outputs based on inputs (like in supervised learning), the goal is to find inherent structures,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.How does K-Means clustering algorithm work?\n",
        "\n",
        "K-Means aims to partition data into K distinct, non-overlapping clusters. It works iteratively:\n",
        "Initialize: Choose K initial points as cluster centroids (centers), often randomly or using methods like K-Means++.\n",
        "Assign: Assign each data point to the cluster whose centroid is nearest (usually based on Euclidean distance).\n",
        "Update: Recalculate the position of each centroid by taking the mean of all data points assigned to its cluster.\n",
        "Repeat: Repeat the Assignment and Update steps until the centroids stabilize (stop moving significantly) or a maximum number of iterations is reached."
      ],
      "metadata": {
        "id": "be16FzRGFgEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Explain the concept of a dendrogram in hierarchical clustering.\n",
        "\n",
        "A dendrogram is a tree diagram used to visualize the output of hierarchical clustering. It illustrates how clusters are successively merged (in agglomerative clustering) or split (in divisive clustering). The vertical axis typically represents the distance or dissimilarity at which clusters are joined. By cutting the dendrogram horizontally at a certain height, you can obtain a specific number of clusters"
      ],
      "metadata": {
        "id": "8jk_Y2idFgA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.What is the main difference between K-Means and Hierarchical Clustering?\n",
        "\n",
        "Approach: K-Means partitions data into a pre-defined number (K) of flat clusters simultaneously. Hierarchical clustering builds a nested hierarchy of clusters, either bottom-up (agglomerative) or top-down (divisive).\n",
        "Number of Clusters (K): K-Means requires K to be specified beforehand. Hierarchical clustering does not require K upfront; you can choose the number of clusters later by interpreting the dendrogram.\n",
        "Output: K-Means outputs the final cluster assignments. Hierarchical clustering outputs a dendrogram representing the hierarchy"
      ],
      "metadata": {
        "id": "BC0IaZU9Ff-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.What are the advantages of DBSCAN over K-Means?\n",
        "\n",
        "DBSCAN can find clusters of arbitrary shapes, unlike K-Means which assumes spherical clusters.\n",
        "DBSCAN can identify and handle noise points (outliers), whereas K-Means assigns every point to a cluster.\n",
        "DBSCAN doesn't require the number of clusters to be specified beforehand (though it needs eps and minPts parameters)."
      ],
      "metadata": {
        "id": "vGAllS9uFf7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.When would you use Silhouette Score in clustering?\n",
        "\n",
        "The Silhouette Score is used to evaluate the quality of a clustering result. It measures how well-separated clusters are and how cohesive points are within their own cluster. You use it to:\n",
        "Assess how appropriate the clustering is for the data.\n",
        "Compare results from different clustering algorithms or different parameter settings (like choosing the optimal K for K-Means)."
      ],
      "metadata": {
        "id": "gEi0wp4KFf34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.What are the limitations of Hierarchical Clustering?\n",
        "\n",
        "Scalability: It can be computationally expensive, especially for large datasets (often O(n²) or O(n³) time complexity).\n",
        "Irreversibility: Decisions made early in the merging/splitting process cannot be undone later.\n",
        "Sensitivity: Can be sensitive to the choice of linkage criteria and distance metric.\n",
        "No Objective Function: Unlike K-Means (which minimizes inertia), there isn't a single global objective function being optimized directly in the standard algorithm."
      ],
      "metadata": {
        "id": "zW1PaTYeFf06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Why is feature scaling important in clustering algorithms like K-Means?\n",
        "\n",
        "K-Means relies on distance calculations. If features have very different scales (e.g., age in years vs. income in dollars), the feature with the larger range will disproportionately influence the distance calculation, potentially leading to biased or poor clustering. Scaling features (e.g., using Standardization or Normalization) brings them to a comparable range, ensuring all features contribute more equally."
      ],
      "metadata": {
        "id": "7vvgPPSkFfxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.How does DBSCAN identify noise points?\n",
        "\n",
        "DBSCAN identifies points as noise if they are not part of any cluster. A point is part of a cluster if it's a core point (has at least minPts neighbors within distance eps) or is density-reachable from a core point (connected to a core point through a chain of other core points). Any point that doesn't meet these criteria is labeled as noise"
      ],
      "metadata": {
        "id": "6-0MN3MyFft6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Define inertia in the context of K-Means.\n",
        "\n",
        "Inertia, or Within-Cluster Sum of Squares (WCSS), measures the compactness of clusters in K-Means. It's the sum of the squared distances between each data point and the centroid of its assigned cluster. K-Means tries to find cluster centroids that minimize this value."
      ],
      "metadata": {
        "id": "ihnsS0IMFfq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.What is the elbow method in K-Means clustering?\n",
        "\n",
        "The elbow method is a heuristic technique to help select the optimal number of clusters (K). It involves running K-Means for a range of K values and plotting the inertia (WCSS) for each K. The plot typically shows inertia decreasing as K increases. The \"elbow\" point on the plot, where the rate of decrease sharply slows down, is often suggested as a good value for K."
      ],
      "metadata": {
        "id": "FkfLqieWFfoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Describe the concept of \"density\" in DBSCAN.\n",
        "\n",
        "Density in DBSCAN is defined locally around each point based on two parameters: eps (a radius) and minPts (minimum number of points). A region is considered \"dense\" if it contains at least minPts points within a radius of eps. DBSCAN groups together points that are in dense regions, separating them from sparser regions (often containing noise)."
      ],
      "metadata": {
        "id": "93y6ArhPFfla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "Yes, but standard distance metrics like Euclidean distance are not suitable for categorical data. You need to use appropriate distance measures like Hamming distance, Jaccard index, or Gower's distance (for mixed data types). Alternatively, categorical data can be encoded numerically (e.g., one-hot encoding), but this can sometimes pose challenges."
      ],
      "metadata": {
        "id": "IJbOML1TFfiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14.What does a negative Silhouette Score indicate?\n",
        "\n",
        "A negative Silhouette Score for a data point means it is likely assigned to the wrong cluster. It indicates that the average distance to points in its own cluster is greater than the average distance to points in the nearest neighboring cluster. An overall negative average score suggests a poor clustering structure."
      ],
      "metadata": {
        "id": "ck3t19T7FffM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "\n",
        "Linkage criteria define how the distance between two clusters (each potentially containing multiple points) is calculated when deciding which clusters to merge in agglomerative clustering. Common types include:\n",
        "Single: Minimum distance between any point in one cluster and any point in the other.\n",
        "Complete: Maximum distance between any point in one cluster and any point in the other.\n",
        "Average: Average distance between all pairs of points (one from each cluster).\n",
        "Ward: Minimizes the increase in total within-cluster variance after merging."
      ],
      "metadata": {
        "id": "qBOz0CiHFfdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "\n",
        "K-Means implicitly assumes clusters are roughly spherical and equally sized because it uses the mean (centroid) to represent a cluster and partitions space based on distance to these centroids. It struggles when clusters have very different sizes, densities, or non-convex shapes, as the centroids may not accurately represent the cluster centers, and boundaries might be drawn incorrectly."
      ],
      "metadata": {
        "id": "x_cgP6LmFfZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "\n",
        "eps (epsilon): The maximum distance defining a neighborhood. Smaller eps leads to smaller, denser clusters (more noise); larger eps leads to larger clusters (potentially merging distinct ones).\n",
        "minPts: The minimum number of points required to form a dense region (i.e., to define a core point). Lower minPts allows sparser clusters; higher minPts requires higher density, making it more noise-robust but potentially missing smaller clusters."
      ],
      "metadata": {
        "id": "mDAbWxUJFfWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.How does K-Means++ improve upon standard K-Means initialization?\n",
        "\n",
        "K-Means++ provides a smarter way to choose the initial centroids. Instead of purely random selection, it picks the first centroid randomly and then selects subsequent centroids with a probability proportional to the squared distance from the nearest already chosen centroid. This tends to spread out the initial centroids, leading to faster convergence and often better final clustering results compared to random initialization."
      ],
      "metadata": {
        "id": "FLjF_pIxFfUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.What is agglomerative clustering?\n",
        "\n",
        "Agglomerative clustering is the \"bottom-up\" approach to hierarchical clustering. It starts with each data point as an individual cluster. Then, in each step, it merges the two closest clusters based on the chosen linkage criterion until only one cluster (containing all points) remains."
      ],
      "metadata": {
        "id": "IGHujEiKFfR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "Inertia only measures cluster compactness (within-cluster similarity) and always decreases as the number of clusters (K) increases. This makes it hard to use inertia alone to choose K, as it would favor K=N (number of data points). The Silhouette Score considers both cohesion (within-cluster similarity) and separation (between-cluster dissimilarity). It provides a more balanced view of cluster quality, rewarding groupings that are both dense and well-separated from each other, making it more useful for comparing different K values or algorithms."
      ],
      "metadata": {
        "id": "reCZ-7U1Ha7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL_QUESTIONS"
      ],
      "metadata": {
        "id": "c3vJvgXuH4Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "# Plot the cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.7, label='Centroids')\n",
        "plt.title('K-Means Clustering on Synthetic Blobs Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Un0idfPJH4CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Load the iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "y_agg = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Display the first 10 predicted labels\n",
        "print(\"First 10 predicted labels:\", y_agg[:10])"
      ],
      "metadata": {
        "id": "jJ1dkmM5H3_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters and outliers\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Synthetic Moons Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Cluster Label (-1 indicates outliers)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCipEEjTH38w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Print the size of each cluster\n",
        "cluster_sizes = np.bincount(y_kmeans)\n",
        "print(\"Cluster sizes:\", cluster_sizes)"
      ],
      "metadata": {
        "id": "YhLGpAdhH35x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Synthetic Circles Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Cluster Label (-1 indicates outliers)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EaZSj3kvH327"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Output the cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "print(\"Cluster centroids (scaled):\\n\", centroids"
      ],
      "metadata": {
        "id": "JQV2W0qGH30e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data with varying standard deviations\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Blobs with Varying Standard Deviations')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Cluster Label (-1 indicates outliers)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dTmns47AH3yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Apply PCA for dimensionality reduction to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis')\n",
        "plt.title('K-Means Clustering on Digits Data (PCA reduced to 2D)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U53EmpYPH3wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Calculate silhouette scores for different values of k\n",
        "silhouette_scores = []\n",
        "for n_clusters in range(2, 6):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Display the silhouette scores as a bar chart\n",
        "k_values = range(2, 6)\n",
        "plt.bar(k_values, silhouette_scores)\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MX5WgYX3H3t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Load the iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply hierarchical clustering with average linkage\n",
        "linked = linkage(X, 'average')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked,\n",
        "            orientation='top',\n",
        "            labels=iris.target_names[iris.target],\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Average Linkage)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Clustering Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s9EbcutdH3pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data with overlapping clusters\n",
        "X, y = make_blobs(n_samples=300, centers=[[2, 2], [3, 3], [2.5, 2.5]], cluster_std=0.8, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Visualize the decision boundaries\n",
        "h = .02  # step size of the meshgrid\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.imshow(Z, interpolation='nearest',\n",
        "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "           cmap=plt.cm.viridis,\n",
        "           aspect='auto', origin='lower', alpha=0.3)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', edgecolors='k')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
        "plt.title('K-Means Clustering with Decision Boundaries (Overlapping Blobs)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RMU5PQ_iH3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Load the Digits dataset and apply DBSCAN after reducing dimensionality with t-SNE. Visualize the results.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Apply t-SNE for dimensionality reduction to 2 components\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Digits Data (t-SNE reduced to 2D)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(label='Cluster Label (-1 indicates outliers)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "51rvbc0_UBHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with complete linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "y_agg = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Complete Linkage) on Synthetic Blobs Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IP1Tj6u4H3hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "# Calculate inertia for different values of k\n",
        "inertia_values = []\n",
        "k_values = range(2, 7)\n",
        "for n_clusters in k_values:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the inertia values\n",
        "plt.plot(k_values, inertia_values, marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia vs. Number of Clusters (K) for Breast Cancer Dataset')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Twk8eVTBH3e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic concentric circles\n",
        "X, y = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with single linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y_agg = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Single Linkage) on Concentric Circles')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "peV4VOzrH3cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Count the number of clusters (excluding noise, which is labeled -1)\n",
        "n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)"
      ],
      "metadata": {
        "id": "-5WFOPPwH3Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37.Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the\n",
        "#data points\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Generate Synthetic Data\n",
        "#    - n_samples: The total number of points to generate.\n",
        "#    - centers:  The number of clusters (3 in this case).\n",
        "#    - n_features: The number of dimensions (2 for a 2D plot).\n",
        "#    - random_state:  To ensure the same data is generated each time.\n",
        "X, y = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42)\n",
        "\n",
        "# 2. Apply KMeans Clustering\n",
        "#    - n_clusters: The number of clusters to form (should match the number of centers in make_blobs).\n",
        "#    - init:  Method for initializing the cluster centers ('k-means++' is a good default).\n",
        "#    - random_state:  For reproducibility.\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "kmeans.fit(X)  # Fit the KMeans model to the data\n",
        "\n",
        "# Get cluster assignments and centers\n",
        "cluster_labels = kmeans.labels_  # Which cluster each point belongs to\n",
        "cluster_centers = kmeans.cluster_centers_  # The coordinates of the cluster centers\n",
        "\n",
        "# 3. Plot the Results\n",
        "#    - Create a scatter plot of the data points, colored by their cluster assignments.\n",
        "#    - Plot the cluster centers as red 'x' markers.\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))  # Set figure size for better visualization\n",
        "\n",
        "# Scatter plot of the data points\n",
        "#  - X[:, 0]:  All rows, first column (x-coordinates)\n",
        "#  - X[:, 1]:  All rows, second column (y-coordinates)\n",
        "#  - c=cluster_labels:  Color the points based on their cluster assignment\n",
        "#  - cmap='viridis':   Use the 'viridis' colormap (you can change this)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.8)\n",
        "\n",
        "# Plot the cluster centers\n",
        "#  - cluster_centers[:, 0]: x-coordinates of the centers\n",
        "#  - cluster_centers[:, 1]: y-coordinates of the centers\n",
        "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
        "            marker='x', s=200, color='red', label='Cluster Centers')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('KMeans Clustering with 3 Clusters')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()  # Show the legend (for the cluster centers label)\n",
        "plt.grid(True)  # Add a grid for easier visualization\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "1BhuwgbYKt96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38.Load the iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Count the number of noise samples (labeled as -1)\n",
        "n_noise = np.sum(y_dbscan == -1)\n",
        "print(\"Number of noise samples identified by DBSCAN:\", n_noise)"
      ],
      "metadata": {
        "id": "hpBo60VTKt7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic non-linearly separable data\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering (note that K-Means might not perform well on non-linear data)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, alpha=0.7, label='Centroids')\n",
        "plt.title('K-Means Clustering on Non-Linearly Separable Moons Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GSxvu-GGKt4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40. Load the Digits dataset, apply PCA to reduce to 3 components, then use K-Means and visualize with a 3D scatter plot.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Apply PCA for dimensionality reduction to 3 components\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clusters in 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y_kmeans, cmap='viridis')\n",
        "\n",
        "# Plot the cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], c='red', marker='X', s=200, alpha=0.7, label='Centroids')\n",
        "\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "ax.set_title('K-Means Clustering on Digits Data (PCA reduced to 3D)')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_sBsWqhCKtzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=5, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Evaluate the clustering using silhouette score\n",
        "silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "print(f\"Silhouette Score for K-Means clustering with 5 clusters: {silhouette_avg:.4f}\")"
      ],
      "metadata": {
        "id": "e4mdMcZ9Ktw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "# Apply PCA for dimensionality reduction to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=2) # Assuming 2 clusters for visualization\n",
        "y_agg = agg_clustering.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Breast Cancer Data (PCA to 2D)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PpbdF6cXKtuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "\n",
        "# Generate noisy circular data\n",
        "X, y = make_circles(n_samples=200, factor=0.5, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize the results side-by-side\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
        "plt.title('K-Means Clustering on Noisy Circles')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Noisy Circles')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lNhxfeA3Ktrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Load the iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply K-Means clustering (assuming 3 clusters as iris has 3 species)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Calculate the silhouette coefficients for each sample\n",
        "silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "# Visualize the silhouette coefficients\n",
        "y_lower = 10\n",
        "for i in range(3):\n",
        "    cluster_silhouette_values = silhouette_values[cluster_labels == i]\n",
        "    cluster_silhouette_values.sort()\n",
        "    size_cluster_i = cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    color = plt.cm.viridis(float(i) / 3)\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                      0, cluster_silhouette_values,\n",
        "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.title(\"Silhouette Plot for K-Means Clustering on Iris Data\")\n",
        "plt.xlabel(\"Silhouette Coefficient Values\")\n",
        "plt.ylabel(\"Cluster\")\n",
        "plt.axvline(x=silhouette_samples(X, cluster_labels).mean(), color=\"red\", linestyle=\"--\", label=\"Average Silhouette Score\")\n",
        "plt.yticks([])  # Clear the y-axis labels / ticks\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rZaRcQNpKtpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with average linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
        "y_agg = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Average Linkage) on Synthetic Blobs Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zINbMU6DKtjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features).\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "# Standardize the features (important for K-Means)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering (assuming 3 clusters as wine has 3 classes)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Create a pandas DataFrame for visualization\n",
        "df = pd.DataFrame(X_scaled[:, :4], columns=feature_names[:4])\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# Visualize the cluster assignments using a seaborn pairplot\n",
        "sns.pairplot(df, hue='Cluster', palette='viridis')\n",
        "plt.suptitle('K-Means Clustering of Wine Data (First 4 Features)', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K6zbj1RlKtgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Generate noisy blobs data\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.0, 1.5], random_state=42)\n",
        "# Introduce some noise points\n",
        "rng = np.random.RandomState(74)\n",
        "noise = rng.rand(30, 2) * 10 - 5\n",
        "X = np.vstack((X, noise))\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Count the number of clusters and noise points\n",
        "n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
        "n_noise = np.sum(y_dbscan == -1)\n",
        "\n",
        "print(f\"Number of identified clusters: {n_clusters}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "# Optional: Visualize the result\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Noisy Blobs')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Cluster Label (-1 indicates noise)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AGe6tWKjKtd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Apply t-SNE for dimensionality reduction to 2 components\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=10) # Assuming 10 clusters for digits\n",
        "y_agg = agg_clustering.fit_predict(X_tsne)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Digits Data (t-SNE to 2D)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCRPqlPLKtai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}