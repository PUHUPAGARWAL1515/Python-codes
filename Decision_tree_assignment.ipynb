{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBOW+DEJqdS55xkmcifyHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/Decision_tree_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txupgveaY-OK"
      },
      "outputs": [],
      "source": [
        "1.What is a Decision Tree, and how does it work?\n",
        "\n",
        "A Decision Tree is a supervised learning algorithm used for both classification and regression. 1  It works by recursively partitioning the data based on the values of features, creating a tree-like structure of decisions.\n",
        "\n",
        "How it works:\n",
        "Feature Selection: The algorithm selects the best feature to split the data based on some criterion (e.g., Information Gain, Gini Impurity).\n",
        "Splitting: The data is divided into subsets based on the values of the chosen feature.\n",
        "Recursion: Steps 1 and 2 are repeated for each subset until a stopping criterion is met (e.g., maximum depth reached, minimum samples in a leaf).\n",
        "Prediction: For classification, the majority class in a leaf node is the prediction. For regression, the average value in a leaf node is the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. What are impurity measures in Decision Trees?\n",
        "\n",
        "Impurity measures evaluate how \"mixed\" the classes are within a subset of data. A pure node (all samples belong to the same class) has low impurity, while a mixed node has high impurity. Common impurity measures:\n",
        "\n",
        "Gini Impurity: Measures the probability of incorrectly classifying a randomly chosen element in the subset.\n",
        "Entropy: Measures the disorder or uncertainty in the subset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "FT81m7PgZoE9",
        "outputId": "65c453fd-be98-4a83-deeb-0d151babfc46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 19) (<ipython-input-1-f968458f6168>, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f968458f6168>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    Information Gain measures the reduction in entropy or Gini impurity achieved by splitting the data on a particular feature. It's used to select the best feature for splitting at each node.\u001b[0m\n\u001b[0m                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3. What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "Gini Impurity = 1 - Σ (p_i)^2\n",
        "Where p_i is the proportion of samples belonging to class i in the subset.\n",
        "\n"
      ],
      "metadata": {
        "id": "H4c4VRYOZoy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What is the mathematical formula for Entropy?\n",
        "\n",
        "Entropy = - Σ (p_i) * log2(p_i)\n",
        "Where p_i is the proportion of samples belonging to class i in the subset.\n",
        "\n"
      ],
      "metadata": {
        "id": "XdehnJd_Zovv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What is the mathematical formula for Entropy?\n",
        "\n",
        "Entropy = - Σ (p_i) * log2(p_i)\n",
        "Where p_i is the proportion of samples belonging to class i in the subset.\n",
        "\n"
      ],
      "metadata": {
        "id": "llke0znLZott"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain measures the reduction in entropy or Gini impurity achieved by splitting the data on a particular feature. It's used to select the best feature for splitting at each node.\n",
        "\n",
        "Information Gain = Impurity(parent) - Σ ( ( |child_v| / |parent| ) * Impurity(child_v) )\n",
        "Where:\n",
        "\n",
        "Impurity is either Entropy or Gini Impurity.\n",
        "parent is the set of samples at the current node.\n",
        "child_v are the subsets of samples created by splitting on feature v.\n",
        "|S| denotes the number of samples in the set S.\n"
      ],
      "metadata": {
        "id": "0aS08l6lZoqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Gini Impurity: Simpler to compute, slightly less accurate in some cases. Tends to favor splits that create more equally sized partitions.\n",
        "Entropy: More computationally expensive, generally considered slightly more accurate. Doesn't have a strong preference for equally sized partitions.\n",
        "In practice, the choice between Gini Impurity and Entropy often doesn't make a big difference in the final tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "S-1oFApJZooM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "Decision Trees aim to find the feature and split point that maximizes information gain (or minimizes impurity) at each node. This process can be viewed as recursively solving an optimization problem. The choice of impurity measure defines the specific objective function being optimized.\n",
        "\n"
      ],
      "metadata": {
        "id": "8Pv82ogQZol0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning stops the tree growth early, before it reaches its full potential. Conditions for stopping include:\n",
        "\n",
        "Maximum depth: Limit the maximum depth of the tree.\n",
        "Minimum samples split: Stop splitting if a node has fewer than a threshold number of samples.\n",
        "Minimum samples leaf: Stop splitting if leaves would have fewer than a threshold number of samples.\n"
      ],
      "metadata": {
        "id": "lS2GD-H_Zoja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. What is Post-Pruning in Decision Trees?\n",
        "\n",
        "Post-pruning grows the full tree and then removes branches or nodes that do not improve performance. Methods include:\n",
        "\n",
        "Cost Complexity Pruning (CCP): Introduces a penalty term for tree complexity and prunes branches that don't reduce error enough to offset the penalty.\n"
      ],
      "metadata": {
        "id": "8jROT5GUahWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "Pre-pruning: Simpler, computationally cheaper, but might stop too early and underfit the data.\n",
        "Post-pruning: More complex, computationally expensive, but can lead to better performance by allowing the tree to explore more possibilities before pruning back.\n"
      ],
      "metadata": {
        "id": "Vsrq-ldCahTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. What is a Decision Tree Regressor?\n",
        "\n",
        "A Decision Tree Regressor is a Decision Tree used for regression tasks. Instead of predicting a class, it predicts a continuous value. The prediction in a leaf node is typically the average value of the target variable for the samples in that leaf.\n",
        "\n"
      ],
      "metadata": {
        "id": "8L7_SUcjaj0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to understand and interpret.\n",
        "Can handle both categorical and numerical data.\n",
        "Non-parametric (no assumptions about the form of the data).\n",
        "Can model non-linear relationships.\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting.\n",
        "Can be unstable (small changes in data can lead to large changes in the tree).\n",
        "Biased towards features with more levels.\n"
      ],
      "metadata": {
        "id": "x_8FnZ5aajyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. How does a Decision Tree handle missing values?\n",
        "\n",
        "Surrogate Splits: Finds other features that have similar splitting patterns to the feature with missing values and uses them for splitting.\n",
        "Imputation: Fills in missing values with the mean, median, or mode before building the tree.\n",
        "Treat Missing as a Separate Category: Create a new category specifically for missing values.\n"
      ],
      "metadata": {
        "id": "xVgzddmhajv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. How does a Decision Tree handle categorical features?\n",
        "\n",
        "One-Hot Encoding: Convert categorical features into numerical using one-hot encoding.\n",
        "Ordinal Encoding: If the categorical feature has an order, assign numerical values based on the order.\n",
        "Binary Splits: For high-cardinality categorical features, consider making binary splits that group categories together.\n"
      ],
      "metadata": {
        "id": "sIeGV35hajtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "Medical diagnosis\n",
        "Credit risk assessment\n",
        "Customer churn prediction\n",
        "Spam detection\n",
        "Recommender systems"
      ],
      "metadata": {
        "id": "T94T2sBIaq8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical_questions:\n"
      ],
      "metadata": {
        "id": "U--BkWHJat40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy*\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhnxBguuyScu",
        "outputId": "893942a2-9ff9-4943-af03-487e7af4e15b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "#feature importances*\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')  # Gini is the default\n",
        "clf.fit(X, y)\n",
        "\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5oB89J_ySaH",
        "outputId": "88943f62-c906-442b-e56b-1426cef50a13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.06405596 0.92261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "#model accuracy*\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (Entropy):\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ow2IqAySXh",
        "outputId": "32d70d87-a159-4194-b214-d650c12b6608"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Entropy): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "#Squared Error (MSE)*\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing  # Example housing dataset\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing(data_only=True)  # Load only data\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "7nN-g9_4ySVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz*\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                         feature_names=iris.feature_names,\n",
        "                         class_names=iris.target_names,\n",
        "                         filled=True, rounded=True,\n",
        "                         special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_tree\") # Save to a file (e.g., iris_tree.pdf)\n",
        "graph # Display in Jupyter Notebook"
      ],
      "metadata": {
        "id": "xvM3V40iySSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "#accuracy with a fully grown tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "acc_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "print(\"Accuracy (Full Tree):\", acc_full)\n",
        "print(\"Accuracy (Depth 3):\", acc_depth3)"
      ],
      "metadata": {
        "id": "I_V7ytMhySQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "#accuracy with a default tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "acc_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "\n",
        "clf_min5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_min5.fit(X_train, y_train)\n",
        "y_pred_min5 = clf_min5.predict(X_test)\n",
        "acc_min5 = accuracy_score(y_test, y_pred_min5)\n",
        "\n",
        "print(\"Accuracy (Default):\", acc_default)\n",
        "print(\"Accuracy (min_samples_split=5):\", acc_min5)"
      ],
      "metadata": {
        "id": "6VkuFASwySOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "#accuracy with unscaled data*\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Unscaled\n",
        "clf_unscaled = DecisionTreeClassifier()\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Scaled\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy (Unscaled):\", acc_unscaled)\n",
        "print(\"Accuracy (Scaled):\", acc_scaled)"
      ],
      "metadata": {
        "id": "eQmeB7FLySLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "#classification\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "\n",
        "print(\"Feature Importance Scores:\", clf.feature_importances_)"
      ],
      "metadata": {
        "id": "w5m1hY3iyUqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python program to train a Decision Tree Classifier and display the feature importance scores*\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing(data_only=True)\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg_"
      ],
      "metadata": {
        "id": "dkrjHS9ryUnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26.Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "#with an unrestricted tree*\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing(data_only=True)\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg_unrestricted = DecisionTreeRegressor()\n",
        "reg_unrestricted.fit(X_train, y_train)\n",
        "y_pred_unrestricted = reg_unrestricted.predict(X_test)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "reg_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "reg_depth5.fit(X_train, y_train)\n",
        "y_pred_depth5 = reg_depth5.predict(X_test)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "\n",
        "print(\"MSE (Unrestricted):\", mse_unrestricted)\n",
        "print(\"MSE (max_depth=5):\", mse_depth5)"
      ],
      "metadata": {
        "id": "HU0XZ7bmyUky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27.* Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "#visualize its effect on accuracy*\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LGL4FyC4yUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "#Recall, and F1-Score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multiclass\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multiclass\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "id": "qFtT4fw25YfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn*\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fz4k5_i05YG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "#for max_depth and min_samples_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': range(1, 10),\n",
        "    'min_samples_split': range(2, 10)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "4N7_ilD35X4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}