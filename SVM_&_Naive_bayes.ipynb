{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxEIXRRJj2jvfFKtldtfG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/SVM_%26_Naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4bTAZTjiOda"
      },
      "outputs": [],
      "source": [
        "What is a Support Vector Machine (SVM)?\n",
        "\n",
        "SVM is a supervised machine learning algorithm used for classification and regression. It aims to find the optimal hyperplane that best separates data points of different classes.\n",
        "What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM: Seeks to find a hyperplane that perfectly separates the data with no misclassifications. It works only if the data is linearly separable.\n",
        "Soft Margin SVM: Allows for some misclassifications to handle non-linearly separable data or noisy data. It introduces a slack variable to control the trade-off between maximizing the margin and minimizing misclassifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM: Seeks to find a hyperplane that perfectly separates the data with no misclassifications. It works only if the data is linearly separable.\n",
        "Soft Margin SVM: Allows for some misclassifications to handle non-linearly separable data or noisy data.\n",
        "\n",
        " It introduces a slack variable to control the trade-off between maximizing the margin and minimizing misclassifications.\n"
      ],
      "metadata": {
        "id": "UvqBgW-3iQt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the mathematical intuition behind SVM?\n",
        "\n",
        "The intuition is to maximize the margin (the distance between the hyperplane and the closest data points of each class). This is achieved by formulating an optimization problem that minimizes the norm of the weight vector while satisfying the constraint that all data points are correctly classified (or within the margin for soft margin)."
      ],
      "metadata": {
        "id": "WOoC4uL7iQrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Lagrange multipliers are used to solve the constrained optimization problem of SVM. They help convert the problem into a dual form, which is easier to solve and allows for the introduction of kernels."
      ],
      "metadata": {
        "id": "3tR1ibjdiQjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What are Support Vectors in SVM?\n",
        "\n",
        "Support vectors are the data points that lie closest to the hyperplane.\n",
        "\n",
        " They are crucial because they determine the position and orientation of the hyperplane. Removing non-support vectors does not affect the hyperplane.\n"
      ],
      "metadata": {
        "id": "KBa66OEDiQg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "SVC is the application of SVM for classification tasks.\n",
        "\n",
        " It finds the best hyperplane to separate data into different classes.\n"
      ],
      "metadata": {
        "id": "HidjcDEZiQdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "SVR is the application of SVM for regression tasks. It finds a function that approximates the target values within a certain margin of tolerance.\n"
      ],
      "metadata": {
        "id": "zs5g_D72iQZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the Kernel Trick in SVM?\n",
        "\n",
        "The kernel trick is a technique that allows SVM to operate in high-dimensional feature spaces without explicitly computing the coordinates of the data in that space.\n",
        "\n",
        " It uses kernel functions to compute the dot products between data points in the high-dimensional space, effectively mapping the data without doing the expensive computation.\n"
      ],
      "metadata": {
        "id": "jfyHdXkCiQW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "\n",
        "Linear Kernel: Suitable for linearly separable data. It's the simplest kernel and performs well when the data has a large number of features.\n",
        "Polynomial Kernel: Maps data to a higher-dimensional space by considering polynomial combinations of the original features. It can capture non-linear relationships but is prone to overfitting with high degrees.\n",
        "RBF Kernel (Radial Basis Function): Maps data to an infinite-dimensional space. It's very flexible and can capture complex non-linear relationships. It's the most commonly used kernel but requires careful tuning of parameters."
      ],
      "metadata": {
        "id": "Ga7rIsyRiQUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the effect of the C parameter in SVM?\n",
        "\n",
        "The C parameter controls the trade-off between maximizing the margin and minimizing the training error.\n",
        "\n",
        " A smaller C creates a wider margin but allows more misclassifications (high bias, low variance), while a larger C aims for fewer misclassifications but can lead to a narrower margin (low bias, high variance)."
      ],
      "metadata": {
        "id": "lKqbLDEiiQRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "The gamma parameter controls the influence of a single training example.\n",
        "\n",
        " A small gamma means a larger similarity radius and more points are considered similar, leading to a smoother decision boundary (high bias, low variance). A large gamma means a smaller similarity radius and only points very close are considered, leading to a more complex decision boundary (low bias, high variance)"
      ],
      "metadata": {
        "id": "XJUaMGu0iQO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes' theorem.\n",
        "\n",
        " It's called \"naïve\" because it assumes that all features are independent of each other given the class label, which is often not true in real-world data."
      ],
      "metadata": {
        "id": "s5Am15cdjFuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is Bayes' Theorem?\n",
        "\n",
        "Bayes' theorem describes the probability of an event based on prior knowledge of conditions that might be related\n",
        "\n",
        " to the event. Mathematically, it's expressed as: P(A|B) = [P(B|A) * P(A)] / P(B)"
      ],
      "metadata": {
        "id": "t7xJG-MJjFrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Gaussian Naïve Bayes: Assumes that the features follow a Gaussian (normal) distribution. Used for continuous data.\n",
        "Multinomial Naïve Bayes: Assumes that the features represent counts or frequencies, such as word counts in text classification. Used for discrete data.\n",
        "Bernoulli Naïve Bayes: Assumes that the features are binary (e.g., presence or absence of a word).\n",
        "\n",
        " Used for binary or boolean features."
      ],
      "metadata": {
        "id": "MvKTrIHOjFoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "Use Gaussian Naïve Bayes when your features are continuous and approximately normally distributed."
      ],
      "metadata": {
        "id": "IXonCXQejFmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "Independence of Features: All features are assumed to be independent of each other given the class label.\n",
        "Equal Importance of Features: All features are considered equally important."
      ],
      "metadata": {
        "id": "UMUgSMcljFi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "Advantages: Simple, fast, effective for high-dimensional data, performs well with categorical inputs, not sensitive to irrelevant features.\n",
        "Disadvantages: Strong independence assumption, can suffer from zero-frequency problem (if a feature value is not seen in training, it gets zero probability), can be less accurate than more complex models."
      ],
      "metadata": {
        "id": "LAKYu3f7jFf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "Naïve Bayes is effective for text classification because it can handle high-dimensional data (text data with many words), is computationally efficient, and often performs well despite the independence assumption.\n",
        "\n"
      ],
      "metadata": {
        "id": "RsPYr9zhjFdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Compare SVM and Naïve Bayes for classification tasks.\n",
        "\n",
        "SVM: More accurate for complex datasets, effective in high-dimensional spaces, works well with clear margin of separation. Can be computationally expensive.\n",
        "Naïve Bayes: Simpler, faster, works well for high-dimensional data, effective for text classification. Makes strong independence assumptions."
      ],
      "metadata": {
        "id": "fAmDBskajFat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "Laplace smoothing (or additive smoothing) is used to address the zero-frequency problem.\n",
        "\n",
        " It adds a small constant (usually 1) to all counts, ensuring that no probability is zero. This prevents the model from assigning zero probability to unseen feature values during prediction."
      ],
      "metadata": {
        "id": "lwm6MtFgjFXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL_QUESTIONS:"
      ],
      "metadata": {
        "id": "cY4wUZXZjFU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#: Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')  # You can change the kernel\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "RelIGyV3mUXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "#compare their accuracies.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Linear Kernel Accuracy: {accuracy_linear}\")\n",
        "\n",
        "# SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf}\")"
      ],
      "metadata": {
        "id": "dGO6NywjmUU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###$: Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "#Squared Error (MSE).\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = datasets.load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVR\n",
        "svr = SVR(kernel='rbf')  # You can change the kernel\n",
        "\n",
        "# Train the regressor\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate using MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ],
      "metadata": {
        "id": "OKIIXMotmUSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##%: Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "#boundary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Iris dataset (using only two features for visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Using only sepal length and sepal width\n",
        "y = iris.target\n",
        "\n",
        "# Create an SVM Classifier with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)  # You can change the degree\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predict the Z values\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary and the data points\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.title('SVM with Polynomial Kernel')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cnGVoyKXmUPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "#evaluate accuracy:\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "AAUn-WtTmUMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "#Newsgroups dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Create a Multinomial Naïve Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "7_m_GsW7nhuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "#boundaries visually\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load Iris dataset (using only two features for visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Define different C values\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X, y)\n",
        "\n",
        "    # Create meshgrid for plotting\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(f'SVM with C={C}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a7HzQfT7nhr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "#binary features.\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample binary dataset\n",
        "X = np.random.randint(2, size=(100, 10))  # 100 samples, 10 binary features\n",
        "y = np.random.randint(2, size=100)  # Binary labels\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train Bernoulli Naïve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "W7DRp5QwnhpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "#unscaled data.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without scaling\n",
        "svm_unscaled = SVC()\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy (Unscaled): {accuracy_unscaled}\")\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with scaling\n",
        "svm_scaled = SVC()\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy (Scaled): {accuracy_scaled}\")"
      ],
      "metadata": {
        "id": "02gnb-A8nhmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#= Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "#after Laplace Smoothing.\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
        "y = np.random.randint(2, size=100)  # Binary labels\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Gaussian Naïve Bayes without Laplace Smoothing\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=0)  # var_smoothing=0 disables smoothing\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "\n",
        "# Gaussian Naïve Bayes with Laplace Smoothing (default)\n",
        "gnb_with_smoothing = GaussianNB()  # Default var_smoothing is 1e-9\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "\n",
        "print(\"Predictions without smoothing:\", y_pred_no_smoothing[:5])\n",
        "print(\"Predictions with smoothing:\", y_pred_with_smoothing[:5])"
      ],
      "metadata": {
        "id": "kdUlf6KEuSC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "#gamma, kernel).\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf', 'linear', 'poly']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "o1TqiqY3uSAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "#check it improve accuracy.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without class weighting\n",
        "svm_no_weight = SVC()\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"Accuracy (No Weight): {accuracy_no_weight}\")\n",
        "\n",
        "# Train SVM with class weighting\n",
        "svm_with_weight = SVC(class_weight='balanced')\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "print(f\"Accuracy (With Weight): {accuracy_with_weight}\")"
      ],
      "metadata": {
        "id": "Fegt7NsRuR9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to implement a Naïve Bayes classifier for spam detection using email data.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups  # Using 20newsgroups as a proxy\n",
        "\n",
        "# Load a sample dataset (replace with your email dataset)\n",
        "categories = ['alt.atheism', 'soc.religion.christian']  # Using these as \"spam\" and \"ham\"\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Train a Multinomial Naïve Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = mnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "A_ejA30YuR66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "#compare their accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset (e.g., Iris)\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "print(f\"Naïve Bayes Accuracy: {accuracy_gnb}\")\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {accuracy_svm}\")"
      ],
      "metadata": {
        "id": "9qVhxR_XuR4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "#results.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset (e.g., Iris)\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes without feature selection\n",
        "gnb_no_selection = GaussianNB()\n",
        "gnb_no_selection.fit(X_train, y_train)\n",
        "y_pred_no_selection = gnb_no_selection.predict(X_test)\n",
        "accuracy_no_selection = accuracy_score(y_test, y_pred_no_selection)\n",
        "print(f\"Accuracy (No Feature Selection): {accuracy_no_selection}\")\n",
        "\n",
        "# Feature selection using SelectKBest\n",
        "selector = SelectKBest(f_classif, k=2)  # Select top 2 features\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train Naïve Bayes with feature selection\n",
        "gnb_selection = GaussianNB()\n",
        "gnb_selection.fit(X_train_selected, y_train)\n",
        "y_pred_selection = gnb_selection.predict(X_test_selected)\n",
        "accuracy_selection = accuracy_score(y_test, y_pred_selection)\n",
        "print(f\"Accuracy (Feature Selection): {accuracy_selection}\")"
      ],
      "metadata": {
        "id": "fbQZwuAMuR1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "#strategies on the Wine dataset and compare their accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVM with OvR (default)\n",
        "svm_ovr = SVC(decision_function_shape='ovr')\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"OvR Accuracy: {accuracy_ovr}\")\n",
        "\n",
        "# SVM with OvO\n",
        "svm_ovo = SVC(decision_function_shape='ovo')\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(f\"OvO Accuracy: {accuracy_ovo}\")"
      ],
      "metadata": {
        "id": "1iT_3NGpuzog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "#Cancer dataset and compare their accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Linear Kernel Accuracy: {accuracy_linear}\")\n",
        "\n",
        "# SVM with Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "print(f\"Polynomial Kernel Accuracy: {accuracy_poly}\")\n",
        "\n",
        "# SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf}\")"
      ],
      "metadata": {
        "id": "xi27L5rmuzl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "#average accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Train SVM with Stratified K-Fold Cross-Validation\n",
        "svm = SVC()\n",
        "scores = cross_val_score(svm, X, y, cv=skf)\n",
        "\n",
        "# Compute average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f\"Average Accuracy: {average_accuracy}\")"
      ],
      "metadata": {
        "id": "k-aloGeZuzjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "#performance.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes with default priors\n",
        "gnb_default = GaussianNB()\n",
        "gnb_default.fit(X_train, y_train)\n",
        "y_pred_default = gnb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "print(f\"Accuracy (Default Priors): {accuracy_default}\")\n",
        "\n",
        "# Train Gaussian Naïve Bayes with custom priors\n",
        "priors = [0.4, 0.3, 0.3]  # Example custom priors (sum must be 1)\n",
        "gnb_custom = GaussianNB(priors=priors)\n",
        "gnb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = gnb_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Accuracy (Custom Priors): {accuracy_custom}\")"
      ],
      "metadata": {
        "id": "y-Kd-kvouzhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "#compare accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without RFE\n",
        "svm_no_rfe = SVC()\n",
        "svm_no_rfe.fit(X_train, y_train)\n",
        "y_pred_no_rfe = svm_no_rfe.predict(X_test)\n",
        "accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)\n",
        "print(f\"Accuracy (No RFE): {accuracy_no_rfe}\")\n",
        "\n",
        "# RFE with SVM\n",
        "rfe = RFE(SVC(), n_features_to_select=10)  # Select top 10 features\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train SVM with RFE\n",
        "svm_rfe = SVC()\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy (RFE): {accuracy_rfe}\")"
      ],
      "metadata": {
        "id": "sf4YpfTwuzfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "#F1-Score instead of accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "rOA4jTgFuzcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "#(Cross-Entropy Loss)\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_prob = gnb.predict_proba(X_test)  # Get predicted probabilities\n",
        "\n",
        "# Calculate Log Loss\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "print(f\"Log Loss: {logloss}\")"
      ],
      "metadata": {
        "id": "_6rBDtoQuzXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Create Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize Confusion Matrix using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P2gbXhWPuzVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "#Error (MAE) instead of MSE.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = datasets.load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")"
      ],
      "metadata": {
        "id": "pvPukhp2uzTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "#score.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_prob = gnb.predict_proba(X_test)[:, 1]  # Get predicted probabilities for positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")"
      ],
      "metadata": {
        "id": "MDS69y1suzM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Load the Breast Cancer dataset (or any binary classification dataset)\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm = SVC(probability=True)  # probability=True is required for predict_proba\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate the area under the Precision-Recall curve (AUC-PR)\n",
        "auc_pr = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f'Precision-Recall Curve (AUC = {auc_pr:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PNyo3PglveAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}