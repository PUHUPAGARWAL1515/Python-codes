{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+nF6/ilGIHbcppkBk7FWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/KNN_%26_PCA_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCj6FNfbM5ph"
      },
      "outputs": [],
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work?\n",
        "\n",
        "KNN is a simple, supervised machine learning algorithm that can be used for both classification and regression tasks. It works by:\n",
        "\n",
        "Storing all available data: The algorithm memorizes the training dataset.\n",
        "Finding the K nearest neighbors: When a new data point needs to be predicted, it finds the 'K' data points in the training set that are closest to it based on a distance metric (e.g., Euclidean distance).\n",
        "Making a prediction:\n",
        "Classification: The class label assigned to the new point is the most frequent class among its K nearest neighbors (majority vote).\n",
        "Regression: The predicted value for the new point is the average (or median) of the values of its K nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. What is the difference between KNN Classification and KNN Regression?\n",
        "\n",
        "KNN Classification: Predicts a categorical label (class). The output is a class membership.\n",
        "KNN Regression: Predicts a continuous value. The output is a numerical value."
      ],
      "metadata": {
        "id": "rAaPTCpsNAsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. What is the role of the distance metric in KNN?\n",
        "\n",
        "The distance metric determines how \"close\" data points are to each other. Common distance metrics include:\n",
        "\n",
        "Euclidean distance: The straight-line distance between two points. (Most common)\n",
        "Manhattan distance: The sum of the absolute differences between the coordinates of two points. (Useful when features are not continuous)\n",
        "Minkowski distance: A generalization of Euclidean and Manhattan distances.\n",
        "The choice of distance metric can significantly impact the performance of the KNN algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7EpTRvjNAqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What is the Curse of Dimensionality in KNN?\n",
        "\n",
        "As the number of features (dimensions) increases, the data becomes increasingly sparse. This means that:\n",
        "\n",
        "Distances become less meaningful: In high-dimensional spaces, the difference between the nearest and farthest points becomes smaller, making it harder to find meaningful neighbors.\n",
        "Computational cost increases: Calculating distances in high-dimensional spaces is more computationally expensive.\n",
        "Overfitting risk increases: The model may overfit to the training data."
      ],
      "metadata": {
        "id": "ULcYbSy5NAna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. How can we choose the best value of K in KNN?\n",
        "\n",
        "Cross-validation: Try different values of K and evaluate the model's performance on a validation set using cross-validation.\n",
        "Rule of thumb: Often, the square root of the number of data points is a good starting point.\n",
        "Domain knowledge: Consider the nature of the data and the problem at hand."
      ],
      "metadata": {
        "id": "zvzW2QMgNAlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. What are KD Tree and Ball Tree in KNN?\n",
        "\n",
        "These are data structures used to efficiently find the nearest neighbors in high-dimensional spaces, addressing the computational cost problem:\n",
        "\n",
        "KD Tree (K-Dimensional Tree): A space-partitioning data structure that recursively divides the data into smaller regions based on feature values.\n",
        "Ball Tree: A space-partitioning data structure that organizes data points into a series of nested hyperspheres (balls)."
      ],
      "metadata": {
        "id": "bMuGQwrjNAiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. When should you use KD Tree vs. Ball Tree?\n",
        "\n",
        "KD Tree: Generally faster for low-dimensional data and when the number of data points is not too large.\n",
        "Ball Tree: Often performs better in high-dimensional spaces and for large datasets. It is more robust to the curse of dimensionality."
      ],
      "metadata": {
        "id": "5meiog7zNAfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. What are the disadvantages of KNN?\n",
        "\n",
        "Computationally expensive: Especially for large datasets, as it needs to calculate distances to all training points.\n",
        "Sensitive to irrelevant features: Features with little predictive power can negatively impact performance.\n",
        "Requires feature scaling: Features with larger scales can dominate the distance calculation.\n",
        "Memory intensive: Needs to store the entire training dataset"
      ],
      "metadata": {
        "id": "dnT879XKNAc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. How does feature scaling affect KNN?\n",
        "\n",
        "Feature scaling (e.g., standardization or normalization) is crucial for KNN because it ensures that all features contribute equally to the distance calculation. Without scaling, features with larger ranges can dominate the distance, leading to biased results"
      ],
      "metadata": {
        "id": "40NnhaNlNAal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. What is PCA (Principal Component Analysis)?\n",
        "\n",
        "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving as much variance as possible. It identifies the principal components, which are orthogonal directions that capture the most variability in the data."
      ],
      "metadata": {
        "id": "RzlLhM6aNAYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. How does PCA work?\n",
        "\n",
        "Standardize the data: Scale the features to have zero mean and unit variance.\n",
        "Compute the covariance matrix: Measures the relationships between features.\n",
        "Find the eigenvectors and eigenvalues of the covariance matrix: Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each principal component.\n",
        "Sort eigenvectors by eigenvalues: Order the eigenvectors in decreasing order of their corresponding eigenvalues.\n",
        "Select the top k eigenvectors: Choose the top k eigenvectors that capture the most variance (k is the desired number of dimensions).\n",
        "Project the data onto the new subspace: Transform the original data onto the subspace spanned by the selected eigenvectors."
      ],
      "metadata": {
        "id": "IXqHzkbrNAS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. What is the geometric intuition behind PCA?\n",
        "\n",
        "PCA tries to find a new set of axes (principal components) that best represent the data. The first principal component is the direction that captures the most variance, the second is orthogonal to the first and captures the second most variance, and so on.  Essentially, it rotates and projects the data to maximize the spread along the new axes."
      ],
      "metadata": {
        "id": "dKbO7j7vNAQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. What is the difference between Feature Selection and Feature Extraction?\n",
        "\n",
        "Feature Selection: Selects a subset of the original features without modifying them. Examples: filter methods, wrapper methods, embedded methods.\n",
        "Feature Extraction: Transforms the original features into a new set of features. Examples: PCA, LDA, autoencoders."
      ],
      "metadata": {
        "id": "Lus7fWTfNANl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. What are Eigenvalues and Eigenvectors in PCA?\n",
        "\n",
        "Eigenvectors: Directions (vectors) that remain unchanged (only scaled) when a linear transformation is applied. In PCA, they represent the principal components.\n",
        "Eigenvalues: Scaling factors that correspond to the eigenvectors. They indicate the amount of variance explained by each principal component."
      ],
      "metadata": {
        "id": "obOeXmYQNALM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. How do you decide the number of components to keep in PCA?\n",
        "\n",
        "Explained variance ratio: Choose the number of components that explain a sufficiently high percentage of the total variance (e.g., 95%).\n",
        "Scree plot: Plot the eigenvalues and look for an \"elbow\" point where the explained variance starts to level off.\n",
        "Cross-validation: Evaluate the model's performance with different numbers of components using cross-validation."
      ],
      "metadata": {
        "id": "TfLtjv34NAI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. Can PCA be used for classification?\n",
        "\n",
        "Yes, PCA can be used as a preprocessing step for classification. It reduces the dimensionality of the data, which can improve the performance of classifiers by reducing noise and overfitting."
      ],
      "metadata": {
        "id": "ToxpLRbBNAGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. What are the limitations of PCA?\n",
        "\n",
        "Linearity: PCA assumes linear relationships between features. It may not be effective for data with non-linear relationships.\n",
        "Loss of information: Dimensionality reduction always involves some loss of information.\n",
        "Sensitive to scaling: Features need to be scaled before applying PCA."
      ],
      "metadata": {
        "id": "Rn4LM2UQNADz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. How do KNN and PCA complement each other?\n",
        "\n",
        "PCA can be used to reduce the dimensionality of the data before applying KNN. This can:\n",
        "\n",
        "Reduce computational cost: KNN is faster in lower dimensions.\n",
        "Improve performance: By removing irrelevant features and noise, PCA can improve the accuracy of KNN."
      ],
      "metadata": {
        "id": "1VwyZ5mCNABR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. How does KNN handle missing values in a dataset?\n",
        "\n",
        "KNN itself doesn't inherently handle missing values. Common approaches include:\n",
        "\n",
        "Imputation: Replace missing values with estimates (e.g., mean, median, mode).\n",
        "Distance calculation with missing values: Modify the distance metric to handle missing values (e.g., ignore missing values or assign a penalty).\n",
        "Removing rows/columns with missing values: This can lead to loss of information."
      ],
      "metadata": {
        "id": "RFh8mutEOu4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "\n",
        "PCA (Unsupervised): Focuses on finding the directions that maximize variance in the data, regardless of class labels.\n",
        "LDA (Supervised): Focuses on finding the directions that maximize the separation between different classes. It takes class labels into account."
      ],
      "metadata": {
        "id": "Z_3x0dfNOu2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL_QUESTIONS:"
      ],
      "metadata": {
        "id": "BRSQi4CuOuzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train a KNN Classifier on the Iris dataset and print model accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a KNN classifier (K=3 is a common starting point)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "0zmi2-NoOuw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a KNN regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=3)\n",
        "\n",
        "# Train the model\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "id": "62wkmdTROuuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(\"Accuracy (Euclidean):\", accuracy_euclidean)\n",
        "\n",
        "# Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(\"Accuracy (Manhattan):\", accuracy_manhattan)"
      ],
      "metadata": {
        "id": "HX4jNImvOuqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a KNN Classifier with different values of K and visualize decision boundaries.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# ... (Load Iris dataset and split as in #21) ...\n",
        "\n",
        "# Visualize decision boundaries for different K values\n",
        "k_values = [1, 3, 5, 10]\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, k in enumerate(k_values):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train[:, :2], y_train)  # Use only the first two features for visualization\n",
        "\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "    DecisionBoundaryDisplay.from_estimator(knn, X_train[:, :2], cmap=plt.cm.RdYlBu, ax=ax, response_method=\"predict\", plot_method=\"pcolormesh\", shading=\"auto\")\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdYlBu, edgecolor='k', s=20)\n",
        "    plt.title(f\"KNN (K={k}) Decision Boundary\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nw8MNf8QOul6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ... (Load Iris dataset and split as in #21) ...\n",
        "\n",
        "# Unscaled data\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(\"Accuracy (Unscaled):\", accuracy_unscaled)\n",
        "\n",
        "# Scaled data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy (Scaled):\", accuracy_scaled)"
      ],
      "metadata": {
        "id": "S5Gq-ILROujo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a PCA model on synthetic data and print the explained variance ratio for each component.\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create synthetic data (replace with your data)\n",
        "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
        "\n",
        "# Create PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "Ax7Xbd7bTCVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n",
        "# ... (Load Iris dataset and split as in #21) ...\n",
        "\n",
        "# Accuracy without PCA (already calculated in #25)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2) # Reduce to 2 components for simplicity\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"Accuracy (PCA):\", accuracy_pca)"
      ],
      "metadata": {
        "id": "u99NaozyTCRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# ... (Load Iris dataset and split as in #21) ...\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9, 11], 'metric': ['euclidean', 'manhattan']}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "0VfKTvTjTCPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Train a KNN Classifier and check the number of misclassified samples.\n",
        "# ... (Load Iris dataset and split as in #21) ...\n",
        "\n",
        "# Train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate misclassified samples\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "print(\"Number of Misclassified Samples:\", misclassified)"
      ],
      "metadata": {
        "id": "Tqv339iCTCMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Train a PCA model and visualize the cumulative explained variance.\n",
        "# ... (Create synthetic data as in #26) ...\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.plot(cumulative_variance)\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rn7JeiUhTCJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare\n",
        "#accuracy.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_weighting():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    knn_uniform = KNeighborsClassifier(weights='uniform')\n",
        "    knn_distance = KNeighborsClassifier(weights='distance')\n",
        "\n",
        "    knn_uniform.fit(X_train, y_train)\n",
        "    knn_distance.fit(X_train, y_train)\n",
        "\n",
        "    acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test))\n",
        "    acc_distance = accuracy_score(y_test, knn_distance.predict(X_test))\n",
        "\n",
        "    print(f\"Accuracy (uniform): {acc_uniform:.4f}\")\n",
        "    print(f\"Accuracy (distance): {acc_distance:.4f}\")\n",
        "\n",
        "knn_weighting()"
      ],
      "metadata": {
        "id": "bPCmbZriTCHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32.Train a KNN Regressor and analyze the effect of different K values on performance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "def knn_regressor_k():\n",
        "    data = load_diabetes()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    k_values = [1, 3, 5, 10, 20]\n",
        "    mse_values = []\n",
        "\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsRegressor(n_neighbors=k)\n",
        "        knn.fit(X_train, y_train)\n",
        "        mse = mean_squared_error(y_test, knn.predict(X_test))\n",
        "        mse_values.append(mse)\n",
        "\n",
        "    plt.plot(k_values, mse_values, marker='o')\n",
        "    plt.xlabel('K')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.title('KNN Regressor Performance vs. K')\n",
        "    plt.show()\n",
        "\n",
        "knn_regressor_k()"
      ],
      "metadata": {
        "id": "l1lw2_vYTCEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33.Implement KNN Imputation for handling missing values in a dataset\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_imputation():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_with_nan = X.copy()\n",
        "    X_with_nan[np.random.choice([False, True], size=X.shape)] = np.nan\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    X_imputed = imputer.fit_transform(X_with_nan)\n",
        "\n",
        "    print(\"Original data with NaNs (first 5 rows):\\n\", X_with_nan[:5])\n",
        "    print(\"\\nImputed data (first 5 rows):\\n\", X_imputed[:5])\n",
        "\n",
        "knn_imputation()"
      ],
      "metadata": {
        "id": "oNQTLtLLX902"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34.Train a PCA model and visualize the data projection onto the first two principal components\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_visualization():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.title('PCA Projection')\n",
        "    plt.show()\n",
        "\n",
        "pca_visualization()"
      ],
      "metadata": {
        "id": "p18Je_7JX9wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35.Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_tree_algorithms():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    knn_kd = KNeighborsClassifier(algorithm='kd_tree')\n",
        "    knn_ball = KNeighborsClassifier(algorithm='ball_tree')\n",
        "\n",
        "    knn_kd.fit(X_train, y_train)\n",
        "    knn_ball.fit(X_train, y_train)\n",
        "\n",
        "    acc_kd = accuracy_score(y_test, knn_kd.predict(X_test))\n",
        "    acc_ball = accuracy_score(y_test, knn_ball.predict(X_test))\n",
        "\n",
        "    print(f\"Accuracy (KD Tree): {acc_kd:.4f}\")\n",
        "    print(f\"Accuracy (Ball Tree): {acc_ball:.4f}\")\n",
        "\n",
        "knn_tree_algorithms()"
      ],
      "metadata": {
        "id": "wPPaSrVkX9uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36.Train a PCA model on a high-dimensional dataset and visualize the Scree plot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_scree_plot():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    pca = PCA()\n",
        "    pca.fit(X_scaled)\n",
        "\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title('Scree Plot')\n",
        "    plt.show()\n",
        "\n",
        "pca_scree_plot()"
      ],
      "metadata": {
        "id": "GWTBuSeEX9r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37.Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score4\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_performance_metrics():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "knn_performance_metrics()"
      ],
      "metadata": {
        "id": "ThjZVF1PX9pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38.Train a PCA model and analyze the effect of different numbers of components on accuracy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_components_accuracy():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_scaled_train = StandardScaler().fit_transform(X_train)\n",
        "    X_scaled_test = StandardScaler().transform(X_test)\n",
        "\n",
        "    accuracies = []\n",
        "    n_components_range = range(1, X_train.shape[1] + 1)\n",
        "\n",
        "    for n_components in n_components_range:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_train_pca = pca.fit_transform(X_scaled_train)\n",
        "        X_test_pca = pca.transform(X_scaled_test)\n",
        "\n",
        "        knn = KNeighborsClassifier()\n",
        "        knn.fit(X_train_pca, y_train)\n",
        "        acc = accuracy_score(y_test, knn.predict(X_test_pca))\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    plt.plot(n_components_range, accuracies, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Number of PCA Components')\n",
        "    plt.show()\n",
        "\n",
        "pca_components_accuracy()"
      ],
      "metadata": {
        "id": "gwr-G9bYX9ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39.Train a KNN Classifier with different leaf_size values and compare accuracy.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_leaf_size():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    leaf_sizes = [10, 20, 30, 40, 50]\n",
        "    accuracies = []\n",
        "\n",
        "    for leaf_size in leaf_sizes:\n",
        "        knn = KNeighborsClassifier(leaf_size=leaf_size)\n",
        "        knn.fit(X_train, y_train)\n",
        "        acc = accuracy_score(y_test, knn.predict(X_test))\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    plt.plot(leaf_sizes, accuracies, marker='o')\n",
        "    plt.xlabel('Leaf Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Leaf Size')\n",
        "    plt.show()\n",
        "\n",
        "knn_leaf_size()"
      ],
      "metadata": {
        "id": "3IjBLFv-X9j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40.Train a PCA model and visualize how data points are transformed before and after PCA.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_transformation_visualization():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y)\n",
        "    plt.title('Original Data (First 2 Features)')\n",
        "    plt.xlabel('Feature 1 (Scaled)')\n",
        "    plt.ylabel('Feature 2 (Scaled)')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "    plt.title('Data After PCA (First 2 Components)')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.show()\n",
        "\n",
        "pca_transformation_visualization()"
      ],
      "metadata": {
        "id": "V_aB0afbX9hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41.Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "def knn_wine_classification_report():\n",
        "    data = load_wine()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "knn_wine_classification_report()"
      ],
      "metadata": {
        "id": "g_e5GU7TX9fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42.Train a KNN Regressor and analyze the effect of different distance metrics on prediction error4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "def knn_regressor_distance_metrics():\n",
        "    data = load_diabetes()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    distance_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
        "    mse_values = []\n",
        "\n",
        "    for metric in distance_metrics:\n",
        "        knn = KNeighborsRegressor(metric=metric)\n",
        "        knn.fit(X_train, y_train)\n",
        "        mse = mean_squared_error(y_test, knn.predict(X_test))\n",
        "        mse_values.append(mse)\n",
        "\n",
        "    plt.bar(distance_metrics, mse_values)\n",
        "    plt.xlabel('Distance Metric')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.title('KNN Regressor Performance vs. Distance Metric')\n",
        "    plt.show()\n",
        "\n",
        "knn_regressor_distance_metrics()"
      ],
      "metadata": {
        "id": "-B4p0vP6bcig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43.Train a KNN Classifier and evaluate using ROC-AUC score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "def knn_roc_auc():\n",
        "    data = load_breast_cancer()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred_proba = knn.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "knn_roc_auc()"
      ],
      "metadata": {
        "id": "QQ7f4gvTbcgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44.Train a PCA model and visualize the variance captured by each principal component4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_explained_variance():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    pca = PCA()\n",
        "    pca.fit(X_scaled)\n",
        "\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title('Explained Variance vs. Number of Components')\n",
        "    plt.show()\n",
        "\n",
        "pca_explained_variance()"
      ],
      "metadata": {
        "id": "zaGpitkSbcbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45.Train a KNN Classifier and perform feature selection before training\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_feature_selection():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Feature selection\n",
        "    selector = SelectKBest(f_classif, k=2)  # Select top 2 features\n",
        "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "\n",
        "    # Train KNN\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(X_train_selected, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, knn.predict(X_test_selected))\n",
        "    print(f\"Accuracy with feature selection: {accuracy:.4f}\")\n",
        "\n",
        "knn_feature_selection()"
      ],
      "metadata": {
        "id": "kxX_t4bybxGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46.Train a PCA model and visualize the data reconstruction error after reducing dimensions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_reconstruction_error():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # Train PCA with reduced number of components\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Reconstruct data\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "    # Calculate reconstruction error\n",
        "    error = mean_squared_error(X_scaled, X_reconstructed)\n",
        "    print(f\"Reconstruction error: {error:.4f}\")\n",
        "\n",
        "    # Visualize original vs reconstructed\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y)\n",
        "    plt.title('Original Data (Scaled)')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], c=y)\n",
        "    plt.title('Reconstructed Data')\n",
        "    plt.show()\n",
        "\n",
        "pca_reconstruction_error()"
      ],
      "metadata": {
        "id": "eBMCEpIIcojC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47.Train a KNN Classifier and visualize the decision boundary4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def knn_decision_boundary():\n",
        "    data = load_iris()\n",
        "    X, y = data.data[:, :2], data.target  # Use only the first two features\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Create meshgrid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    # Predict on meshgrid\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
        "    plt.title('KNN Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "knn_decision_boundary()"
      ],
      "metadata": {
        "id": "TX2bt5vQcogG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#48.Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def pca_components_data_variance():\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    variances = []\n",
        "    n_components_range = range(1, X_train.shape[1] + 1)\n",
        "\n",
        "    for n_components in n_components_range:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_pca = pca.fit_transform(X_scaled)\n",
        "        variances.append(np.var(X_pca))  # Calculate variance of transformed data\n",
        "\n",
        "    plt.plot(n_components_range, variances, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Variance')\n",
        "    plt.title('Variance vs. Number of PCA Components')\n",
        "    plt.show()\n",
        "\n",
        "pca_components_data_variance()"
      ],
      "metadata": {
        "id": "Qz5PXzqccoc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdTVVGbqcoaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}