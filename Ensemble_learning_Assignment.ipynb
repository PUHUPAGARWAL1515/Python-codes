{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwJ3vWBNL5wUwqUgJp6SMl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/Ensemble_learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHv3DZuY1oG1"
      },
      "outputs": [],
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "Yes, absolutely. Bagging (Bootstrap Aggregating) is a general ensemble technique that can be applied to both classification and regression problems. When used for regression, it's typically called \"Bagging Regressor\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "Single Model Training: Involves training a single model on the entire dataset. The model's performance is highly dependent on the characteristics of the data and the chosen algorithm.\n",
        "Multiple Model Training (Ensemble Learning): Involves training multiple models (often the same type) on different subsets of the data or with different variations. The predictions from these models are then combined (e.g., averaging, voting) to produce a final prediction. This approach aims to improve overall performance and robustness."
      ],
      "metadata": {
        "id": "ZRBX9eGI1ssL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "In Random Forest, feature randomness (also called \"feature bagging\" or \"random subspace method\") means that each tree in the forest is built using a random subset of the features (predictors) at each node split. This ensures that the trees are decorrelated, reducing variance and making the model more robust.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HtPi_4j81spQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "In Bagging (and Random Forest), each tree is trained on a bootstrap sample (sampled with replacement). This means some data points are left out (not included in the bootstrap sample). These left-out samples are called \"out-of-bag\" (OOB) samples. The OOB score is calculated by predicting the target variable for each OOB sample using the trees that didn't include that sample in their training. This provides an unbiased estimate of the model's performance without needing a separate validation set"
      ],
      "metadata": {
        "id": "3x0BNxYY1smc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Random Forest provides two main ways to measure feature importance:\n",
        "\n",
        "Mean Decrease Impurity (Gini Importance): Measures how much each feature contributes to the homogeneity of the nodes and leaves in the trees. Features that lead to larger decreases in impurity are considered more important.\n",
        "Mean Decrease Accuracy: Measures how much the model's accuracy decreases when a feature is randomly permuted. Features that lead to larger decreases in accuracy are considered more important."
      ],
      "metadata": {
        "id": "S9LMEqzS1skF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "Bootstrap Sampling: Create multiple bootstrap samples (random samples with replacement) from the training data.\n",
        "Model Training: Train a base classifier (e.g., Decision Tree) on each bootstrap sample.\n",
        "Aggregation: Combine the predictions of the individual classifiers using voting (for classification) or averaging (for regression). The class with the most votes is the final prediction."
      ],
      "metadata": {
        "id": "rsc1XnDD1shL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. How do you evaluate a Bagging Classifier's performance?\n",
        "\n",
        "OOB Score: As mentioned earlier, the OOB score provides an estimate of the model's performance.\n",
        "Holdout Validation: Split the data into training and testing sets. Train the Bagging Classifier on the training set and evaluate its performance on the testing set using metrics like accuracy, precision, recall, F1-score, etc.\n",
        "Cross-Validation: Use k-fold cross-validation to get a more robust estimate of the model's performance."
      ],
      "metadata": {
        "id": "LU5YgE5B1sLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. How does a Bagging Regressor work?\n",
        "\n",
        "The process is very similar to a Bagging Classifier:\n",
        "\n",
        "Bootstrap Sampling: Create multiple bootstrap samples from the training data.\n",
        "Model Training: Train a base regressor (e.g., Decision Tree Regressor) on each bootstrap sample.\n",
        "Aggregation: Combine the predictions of the individual regressors by averaging them. The average prediction is the final prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "dKsyN9341sIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "The main advantage is improved predictive performance and robustness. Ensemble methods can reduce variance, bias, or both, leading to better generalization on unseen data."
      ],
      "metadata": {
        "id": "Ur6B31Da1sCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "The main challenge is increased complexity and computational cost. Training and managing multiple models can be more resource-intensive compared to training a single model"
      ],
      "metadata": {
        "id": "iDpSxL5M2TJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "The key idea is to combine the strengths of multiple models to achieve better performance than any single model could achieve on its own. This is based on the principle that \"wisdom of the crowd\" is often more accurate than the opinion of a single expert."
      ],
      "metadata": {
        "id": "3F2nmqsm2THA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning method that builds multiple decision trees on random subsets of the data and features. The final prediction is made by aggregating the predictions of the individual trees using voting."
      ],
      "metadata": {
        "id": "yai9YDaQ2TE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. What are the main types of ensemble techniques?\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Trains multiple independent models on bootstrap samples and aggregates their predictions.\n",
        "Boosting: Trains models sequentially, with each model focusing on correcting the errors of the previous models.\n",
        "Stacking: Combines the predictions of multiple diverse models by training a meta-model on their outputs."
      ],
      "metadata": {
        "id": "mzUm-xIb2TBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        "Ensemble learning is a machine learning technique that combines the predictions of multiple models to improve overall performance"
      ],
      "metadata": {
        "id": "_PSbtsGZ2S-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "When computational resources are limited: Ensemble methods can be computationally expensive.\n",
        "When interpretability is crucial: Ensemble models can be harder to interpret than single models.\n",
        "When the base models are already performing very well: The improvement from ensemble methods might be marginal."
      ],
      "metadata": {
        "id": "REI4zP7E2S8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Bagging reduces overfitting by reducing the variance of the model. By training multiple models on different subsets of the data and averaging their predictions, Bagging smooths out the predictions and reduces the impact of noisy data points."
      ],
      "metadata": {
        "id": "8R2aFy062S5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "Reduced Overfitting: Random Forest reduces overfitting by using multiple trees and feature randomness.\n",
        "Improved Accuracy: Aggregating predictions from multiple trees leads to higher accuracy.\n",
        "Robustness: Random Forest is more robust to noisy data and outliers."
      ],
      "metadata": {
        "id": "avE8Db6B2S2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Bootstrap sampling is crucial in Bagging because it creates diverse training datasets for each model. This diversity is essential for reducing variance and improving the overall performance of the ensemble."
      ],
      "metadata": {
        "id": "VfmAlRL02Sz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Image Recognition: Used in tasks like object detection and image classification.\n",
        "Fraud Detection: Helps identify fraudulent transactions by combining multiple models.\n",
        "Medical Diagnosis: Used to improve the accuracy of disease diagnosis.\n",
        "Natural Language Processing: Improves performance in tasks like sentiment analysis and text classification.\n",
        "Recommendation Systems: Provides more accurate recommendations by combining multiple models."
      ],
      "metadata": {
        "id": "BNLt20SR2SxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Bagging: Trains independent models in parallel, focuses on reducing variance.\n",
        "Boosting: Trains models sequentially, with each model focusing on correcting the errors of the previous models, focuses on reducing bias"
      ],
      "metadata": {
        "id": "rNt7_TN_2SvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical_Questions"
      ],
      "metadata": {
        "id": "nv9hs39l3RbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "zRleGdQz3XPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees as base estimators\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print the MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse}\")"
      ],
      "metadata": {
        "id": "2AEQ5Tqy3XMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "for feature_name, importance in zip(data.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance}\")"
      ],
      "metadata": {
        "id": "AZ_egeFZ3XKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train and evaluate Random Forest\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse}\")\n",
        "\n",
        "# Train and evaluate Decision Tree\n",
        "dt_reg.fit(X_train, y_train)\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "print(f\"Decision Tree Regressor MSE: {dt_mse}\")"
      ],
      "metadata": {
        "id": "y0AD4-G73XIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with oob_score=True\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"Random Forest OOB Score: {rf_clf.oob_score_}\")"
      ],
      "metadata": {
        "id": "F8Q85YRi3XF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26.Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as base estimator\n",
        "bagging_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging SVM Classifier Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "lG-G3f493XC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27.Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate Random Forest with different numbers of trees\n",
        "for n_estimators in [50, 100, 200, 300]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest with {n_estimators} trees accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "g3lv_QtS3W_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as base estimator\n",
        "bagging_lr = BaggingClassifier(base_estimator=LogisticRegression(solver='liblinear'), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = bagging_lr.predict_proba(X_test)[:, 1]  # Get probability of positive class\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Bagging Logistic Regression AUC: {auc}\")"
      ],
      "metadata": {
        "id": "RQtnQqs-3W9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29.Train a Random Forest Regressor and analyze feature importance scores.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Analyze and print feature importance scores\n",
        "feature_importances = rf_reg.feature_importances_\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i+1}: {importance}\")"
      ],
      "metadata": {
        "id": "Tvqc10Lw4-G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "\n",
        "# Train and evaluate Random Forest Classifier\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")"
      ],
      "metadata": {
        "id": "PrW614br4-EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "QQpEpaVU4-Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different numbers of estimators\n",
        "for n_estimators in [10, 50, 100, 200]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {n_estimators} estimators MSE: {mse}\")"
      ],
      "metadata": {
        "id": "L0HwTbwF49-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33. Train a Random Forest Classifier and analyze misclassified samples.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified = np.where(y_pred != y_test)[0]\n",
        "\n",
        "# Print the indices of misclassified samples\n",
        "print(\"Indices of misclassified samples:\", misclassified)"
      ],
      "metadata": {
        "id": "dplvVDbj497v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "\n",
        "# Train and evaluate Decision Tree Classifier\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy}\")"
      ],
      "metadata": {
        "id": "mwD6W7ay495R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Train a Random Forest Classifier and visualize the confusion matrix.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Create and visualize the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ifi2hFuL493D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Train Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(random_state=42)),\n",
        "    ('lr', LogisticRegression(solver='liblinear', random_state=42))\n",
        "]\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "BlQMvdaT49zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#. Train a Random Forest Classifier and print the top 5 most important features.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state"
      ],
      "metadata": {
        "id": "LIWnGrnC49wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "id": "2_pTdNB549s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Lists to store results\n",
        "max_depths = [None, 5, 10, 15, 20]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate Random Forest with different max_depths\n",
        "for max_depth in max_depths:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(max_depths, accuracies, marker='o')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of Max Depth on Random Forest Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r1ktfxTJ6Vbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Regressor with Decision Tree base estimator\n",
        "bagging_dt = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "print(f\"Bagging Regressor (Decision Tree) MSE: {mse_dt}\")\n",
        "\n",
        "# Train and evaluate Bagging Regressor with KNeighbors base estimator\n",
        "bagging_knn = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=100, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "print(f\"Bagging Regressor (KNeighbors) MSE: {mse_knn}\")"
      ],
      "metadata": {
        "id": "NKHon7kI6VVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Random Forest ROC-AUC: {roc_auc}\")"
      ],
      "metadata": {
        "id": "KJXwiO1L6VSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "\n",
        "# Evaluate using cross-validation\n",
        "scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print the cross-validation scores and mean score\n",
        "print(f\"Cross-validation scores: {scores}\")\n",
        "print(f\"Mean cross-validation score: {scores.mean()}\")"
      ],
      "metadata": {
        "id": "e8ka1AMx6VPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A_ovsU4U49qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(solver='liblinear', random_state=42))\n",
        "]\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the Stacking Classifier\n",
        "stacking_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Stacking Classifier\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy}\")\n",
        "\n",
        "# Optionally, you can also train and evaluate a single Random Forest or Logistic Regression\n",
        "# for comparison:\n",
        "\n",
        "# # Train and evaluate a single Random Forest Classifier\n",
        "# rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# rf_clf.fit(X_train, y_train)\n",
        "# rf_pred = rf_clf.predict(X_test)\n",
        "# rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "# print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")\n",
        "\n",
        "# # Train and evaluate a single Logistic Regression Classifier\n",
        "# lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "# lr_clf.fit(X_train, y_train)\n",
        "# lr_pred = lr_clf.predict(X_test)\n",
        "# lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "# print(f\"Logistic Regression Classifier Accuracy: {lr_accuracy}\")"
      ],
      "metadata": {
        "id": "pyAlXcUm6qkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Values of max_samples to test\n",
        "max_samples_values =# Testing different proportions\n",
        "mse_values =\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different max_samples\n",
        "for max_samples in max_samples_values:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=100,\n",
        "        max_samples=max_samples,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_values.append(mse)\n",
        "    print(f\"Bagging Regressor (max_samples={max_samples}) MSE: {mse}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(max_samples_values, mse_values, marker='o')\n",
        "plt.xlabel('Max Samples Proportion')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Bagging Regressor Performance vs. Max Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KSKZYfb_6qhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}