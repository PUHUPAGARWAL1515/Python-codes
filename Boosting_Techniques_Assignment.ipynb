{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTNJcFJZWq0cdniOtNMPiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/Boosting_Techniques_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdeeDmcJij7K"
      },
      "outputs": [],
      "source": [
        "1. What is Boosting in Machine Learning?\n",
        "\n",
        "Boosting is an ensemble learning technique that combines multiple weak learners (models that perform slightly better than random guessing) into a strong learner. It works by sequentially training models, with each model focusing on correcting the errors made by the previous ones. The final prediction is a weighted combination of the predictions from all the models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. How does Boosting differ from Bagging?\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "Trains multiple independent models in parallel, each on a different bootstrap sample (random sampling with replacement) of the training data.\n",
        "Reduces variance and overfitting by averaging or voting the predictions of the individual models.\n",
        "Examples: Random Forest.\n",
        "Boosting:\n",
        "Trains models sequentially, with each model learning from the mistakes of the previous ones.\n",
        "Focuses on reducing bias and improving the overall accuracy of the model.\n",
        "Assigns weights to the training instances, giving more weight to misclassified instances in subsequent iterations.\n",
        "Examples: AdaBoost, Gradient Boosting, XGBoost, CatBoost."
      ],
      "metadata": {
        "id": "Z_eMjtQ9pnlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. What is the key idea behind AdaBoost?\n",
        "\n",
        "AdaBoost (Adaptive Boosting) focuses on instances that are difficult to classify. It assigns weights to each training instance, initially equal. After each weak learner is trained, the weights of misclassified instances are increased, while the weights of correctly classified instances are decreased. This forces subsequent models to pay more attention to the difficult instances."
      ],
      "metadata": {
        "id": "CsX4-Wj0pniJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Explain the working of AdaBoost with an example.\n",
        "\n",
        "Let's say we want to classify points as either red or blue:\n",
        "\n",
        "Initialize weights: Assign equal weights to all data points.\n",
        "Train a weak learner: Train a decision stump (a simple decision tree with one level) on the weighted data.\n",
        "Calculate error: Determine the weighted error of the weak learner.\n",
        "Calculate learner weight: Assign a weight to the learner based on its error (higher accuracy = higher weight).\n",
        "Update instance weights: Increase the weights of misclassified instances and decrease the weights of correctly classified instances.\n",
        "Repeat: Go back to step 2 and train another weak learner on the updated weighted data.\n",
        "Combine learners: Combine the predictions of all weak learners, weighted by their learner weights."
      ],
      "metadata": {
        "id": "3mQ086mOpnfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "\n",
        "AdaBoost: Updates instance weights based on misclassifications.\n",
        "Gradient Boosting: Fits new models to the residual errors (the difference between the actual and predicted values) of the previous models. It uses gradient descent to minimize the loss function.\n",
        "Key Difference: Gradient Boosting is more flexible as it can optimize arbitrary differentiable loss functions, while AdaBoost typically uses exponential loss."
      ],
      "metadata": {
        "id": "3Ssxp042pncy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. What is the loss function in Gradient Boosting?\n",
        "\n",
        "The loss function in Gradient Boosting measures the difference between the predicted and actual values. Common loss functions include:\n",
        "\n",
        "Mean Squared Error (MSE): For regression problems.\n",
        "Cross-Entropy (Log Loss): For classification problems.\n",
        "Huber Loss: Robust to outliers."
      ],
      "metadata": {
        "id": "bYNCbJe3pnaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) introduces several improvements:\n",
        "\n",
        "Regularization: Prevents overfitting by adding penalty terms to the loss function.\n",
        "Tree Pruning: Grows trees up to a maximum depth and then prunes back branches that do not improve performance.\n",
        "Parallel Processing: Supports parallel computation for faster training.\n",
        "Handling Missing Values: Can automatically learn the best direction to go when features are missing.\n",
        "Cross-Validation: Built-in cross-validation at each iteration."
      ],
      "metadata": {
        "id": "CE6vTx-3pnXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. What is the difference between XGBoost and CatBoost?\n",
        "\n",
        "XGBoost: Primarily designed for numerical features and requires categorical features to be encoded.\n",
        "CatBoost: Specifically designed to handle categorical features directly, without the need for extensive preprocessing. It uses a novel algorithm called Ordered Boosting to reduce prediction shift caused by target leakage"
      ],
      "metadata": {
        "id": "6vFvm7gWpnU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. What are some real-world applications of Boosting techniques?\n",
        "\n",
        "Search Engine Ranking: Learning to rank search results.\n",
        "Fraud Detection: Identifying fraudulent transactions.\n",
        "Medical Diagnosis: Predicting disease risk and diagnosis.\n",
        "Image Recognition: Object detection and image classification.\n",
        "Natural Language Processing: Sentiment analysis and text classification."
      ],
      "metadata": {
        "id": "d5misSQWpnSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. How does regularization help in XGBoost?\n",
        "\n",
        "Regularization in XGBoost helps prevent overfitting by adding penalty terms to the loss function. This discourages the model from becoming too complex and fitting the noise in the training data. Common regularization techniques include:\n",
        "\n",
        "L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of the weights.\n",
        "L2 Regularization (Ridge): Adds a penalty proportional to the square of the weights."
      ],
      "metadata": {
        "id": "KL2i_NnFrAmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "\n",
        "Number of Estimators (n_estimators): The number of boosting stages.\n",
        "Learning Rate (learning_rate): Controls the contribution of each tree to the final prediction.\n",
        "Maximum Depth (max_depth): Limits the maximum depth of each tree.\n",
        "Minimum Samples Split (min_samples_split): The minimum number of samples required to split an internal node.\n",
        "Subsample (subsample): The fraction of samples used for fitting each tree.\n",
        "Colsample_bytree (colsample_bytree): The fraction of features used for fitting each tree."
      ],
      "metadata": {
        "id": "klTy0nhprAkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. What is the concept of Feature Importance in Boosting?\n",
        "\n",
        "Feature Importance in Boosting refers to the measure of how much each feature contributes to the model's predictions. It helps identify the most relevant features and understand the underlying relationships in the data. Common methods for calculating feature importance include:\n",
        "\n",
        "Gain: Measures the improvement in accuracy brought by a feature to the branches it is on.\n",
        "Cover: Measures the relative number of observations related to the feature.\n",
        "Frequency: Measures the relative number of times a feature is used in the trees."
      ],
      "metadata": {
        "id": "VHHSZum1rAhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. Why is CatBoost efficient for categorical data?\n",
        "\n",
        "CatBoost is efficient for categorical data because:\n",
        "\n",
        "Ordered Boosting: Reduces prediction shift caused by target leakage by using a permutation-driven approach.\n",
        "Handling High Cardinality: Can handle features with a large number of unique categories effectively.\n",
        "Built-in Categorical Feature Support: Does not require explicit encoding of categorical features."
      ],
      "metadata": {
        "id": "kvvg_LhqrAfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL_QUESTIONS:"
      ],
      "metadata": {
        "id": "m8krAbI3rAcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Train an AdaBoost Classifier on a sample dataset and print model accuracy.\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "gmdeMoE-rAZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE).\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "model = AdaBoostRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MAE\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Regressor MAE: {mae}\")"
      ],
      "metadata": {
        "id": "1K7WmlywrAWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance.\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance\n",
        "feature_importance = model.feature_importances_\n",
        "for i, importance in enumerate(feature_importance):\n",
        "    print(f\"Feature {data.feature_names[i]}: {importance}\")"
      ],
      "metadata": {
        "id": "KQlQYWkVrAUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score.\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate R-Squared\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor R-Squared: {r2}\")"
      ],
      "metadata": {
        "id": "TxhdPUNpsEPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting.\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "gb_pred = gb_model.predict(X_test)\n",
        "\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "print(f\"XGBoost Classifier Accuracy: {xgb_accuracy}\")\n",
        "print(f\"Gradient Boosting Classifier Accuracy: {gb_accuracy}\")"
      ],
      "metadata": {
        "id": "0mPy80qvsEMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Train a CatBoost Classifier and evaluate using F1-Score.\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(random_state=42, verbose=0) # verbose=0 to suppress output\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate F1-Score\n",
        "y_pred = model.predict(X_test)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"CatBoost Classifier F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "QkeLQOWKsEHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE).\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"XGBoost Regressor MSE: {mse}\")"
      ],
      "metadata": {
        "id": "Kxx5FE8esEEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train an AdaBoost Classifier and visualize feature importance.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Visualize feature importance\n",
        "feature_importance = model.feature_importances_\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(data.feature_names, feature_importance)\n",
        "plt.xticks(rotation=90)"
      ],
      "metadata": {
        "id": "c-cGsx6JsECC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a Gradient Boosting Regressor and plot learning curves.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Plot learning curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "train_scores_mean = -np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = -np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, label='Training MSE')\n",
        "plt.plot(train_sizes, test_scores_mean, label='Cross-validation MSE')\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1)\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1)\n",
        "plt.xlabel('Training examples')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Learning Curves')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWtQXMzYsD_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Train an XGBoost Classifier and visualize feature importance.\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Visualize feature importance\n",
        "xgb.plot_importance(xgb_model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dDyHX21rsD9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a CatBoost Classifier and plot the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(random_state=42, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LZh82T0OsD6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier with different n_estimators\n",
        "n_estimators_list = [50, 100, 200, 300]\n",
        "accuracy_list = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(n_estimators_list, accuracy_list, marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Accuracy vs. Number of Estimators')\n",
        "plt.xticks(n_estimators_list)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vl4cb0FfsD3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a Gradient Boosting Classifier and visualize the ROC curve.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RkY6U4bysD1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV.\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor and tune learning rate\n",
        "model = xgb.XGBRegressor(random_state=42)\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best learning rate: {grid_search.best_params_['learning_rate']}\")\n",
        "print(f\"Best score: {-grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "id": "XZS2RDMmsDyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weights\n",
        "model_no_weights = CatBoostClassifier(random_state=42, verbose=0)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "accuracy_no_weights = accuracy_score(y_test, y_pred_no_weights)\n",
        "f1_no_weights = f1_score(y_test, y_pred_no_weights)\n",
        "\n",
        "# Calculate class weights\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_sample_weight('balanced', y_train)\n",
        "\n",
        "# Train CatBoost Classifier with class weights\n",
        "model_with_weights = CatBoostClassifier(random_state=42, verbose=0)\n",
        "model_with_weights.fit(X_train, y_train, sample_weight=class_weights)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "accuracy_with_weights = accuracy_score(y_test, y_pred_with_weights)\n",
        "f1_with_weights = f1_score(y_test, y_pred_with_weights)\n",
        "\n",
        "print(f\"Accuracy without class weights: {accuracy_no_weights}\")\n",
        "print(f\"F1-score without class weights: {f1_no_weights}\")\n",
        "print(f\"Accuracy with class weights: {accuracy_with_weights}\")\n",
        "print(f\"F1-score with class weights: {f1_with_weights}\")"
      ],
      "metadata": {
        "id": "W6R_WzztsDv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Train an AdaBoost Classifier and analyze the effect of different learning rates.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier with different learning rates\n",
        "learning_rates =\n",
        "accuracy_list =\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = AdaBoostClassifier(learning_rate=lr, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(learning_rates, accuracy_list, marker='o')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Accuracy vs. Learning Rate')\n",
        "plt.xticks(learning_rates)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mlxWvY0s1hol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30.Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Iris dataset (multi-class)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier for multi-class classification\n",
        "model = xgb.XGBClassifier(objective='multi:softprob', random_state=42)  # Use multi:softprob for multi-class\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions (probabilities)\n",
        "y_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Evaluate using log-loss\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "\n",
        "print(f\"Log-loss: {logloss}\")"
      ],
      "metadata": {
        "id": "SSr-6uOc1hEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}