{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUrYyrdnIxBb+xPTjgBrK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/Evaluation_Metrics_and_Regression_Implementation_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. What is Simple Linear Regression?\n",
        "#Simple Linear Regression is a statistical method used to model the relationship between two variables:\n",
        "\n",
        "Independent Variable (X): This is the variable that we believe is influencing or predicting the other variable. In the code example, study_hours is the independent variable.\n",
        "Dependent Variable (Y): This is the variable we are trying to predict or understand. In the code example, exam_scores is the dependent variable.\n",
        "The Goal:\n",
        "\n",
        "The primary goal of Simple Linear Regression is to find the best-fitting straight line that describes the relationship between these two variables. This line can then be used to predict the value of the dependent variable (Y) based on the value of the independent variable (X).\n",
        "\n",
        "The Equation:\n",
        "\n",
        "The relationship between the variables is represented by a linear equation:\n",
        "\n",
        "\n",
        "Y = mX + c\n",
        "Use code with caution\n",
        "Where:\n",
        "\n",
        "Y: The dependent variable (e.g., exam scores)\n",
        "X: The independent variable (e.g., study hours)\n",
        "m: The slope of the line (how much Y changes for a one-unit change in X)\n",
        "c: The y-intercept (the value of Y when X is 0)\n",
        "How it Works:\n",
        "\n",
        "Simple Linear Regression uses a method called least squares to find the values of 'm' and 'c' that minimize the difference between the predicted values of Y and the actual values of Y in the data. In simpler terms, it finds the line that best fits the data points.\n",
        "\n",
        "In the code example:\n",
        "\n",
        "The code you provided uses the LinearRegression class from the sklearn library to perform Simple Linear Regression. It takes the study_hours and exam_scores as input and finds the best-fitting line. This line is then used to predict exam scores for given study hours, and the results are visualized in a scatter plot with the regression line.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Simple Linear Regression is a powerful tool for understanding and predicting the relationship between two continuous variables. It's widely used in various fields to make predictions, analyze trends, and understand the impact of one variable on another."
      ],
      "metadata": {
        "id": "WSC0HbeYHNzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.What are the assumptions of linear regression?\n",
        "Linear regression models rely on several key assumptions to ensure the validity and reliability of their results. These assumptions are:\n",
        "\n",
        "Linearity: There is a linear relationship between the independent and dependent variables. This means that the change in the dependent variable is proportional to the change in the independent variable.\n",
        "\n",
        "Independence: The observations are independent of each other. This means that the value of one observation does not influence the value of another observation.\n",
        "\n",
        "Normality: The residuals (errors) are normally distributed. This means that the distribution of the differences between the predicted and actual values follows a normal distribution.\n",
        "\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals is the same for all values of the independent variables.\n",
        "\n",
        "No Multicollinearity: The independent variables are not highly correlated with each other. This means that there is no strong linear relationship between any two independent variables.\n",
        "\n",
        "Why are these assumptions important?\n",
        "\n",
        "Violating these assumptions can lead to:\n",
        "\n",
        "Biased or inefficient estimates of the regression coefficients.\n",
        "Inaccurate predictions.\n",
        "Unreliable hypothesis tests and confidence intervals.\n",
        "Misleading conclusions about the relationships between variables.\n"
      ],
      "metadata": {
        "id": "YmxXUYEiHNxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.What is the difference between R-squared and Adjusted R-squared?\n",
        "R-squared\n",
        "\n",
        "Definition: R-squared (RÂ²) measures the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\n",
        "Range: R-squared values range from 0 to 1.\n",
        "Interpretation: A higher R-squared value indicates that the model explains more of the variability in the dependent variable. For example, an R-squared of 0.80 means that 80% of the variation in the dependent variable is explained by the model.\n",
        "Adjusted R-squared\n",
        "\n",
        "Definition: Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "Range: Adjusted R-squared can be negative, but it is usually between 0 and 1.\n",
        "Interpretation: Similar to R-squared, a higher Adjusted R-squared value indicates a better fit. However, it takes into account the complexity of the model (number of predictors), making it a more reliable measure when comparing models with different numbers of predictors.\n",
        "Key Differences\n",
        "\n",
        "Feature\tR-squared\tAdjusted R-squared\n",
        "Considers Model Complexity\tNo\tYes\n",
        "Penalizes Extra Predictors\tNo\tYes\n",
        "Can Increase with More Predictors\tYes, even if they don't improve the model\tNo, it only increases if the new predictor significantly improves the model\n",
        "Used for Model Comparison\tLess reliable when comparing models with different numbers of predictors\tMore reliable for model comparison\n",
        "When to Use Each\n",
        "\n",
        "R-squared: Useful for understanding the overall goodness of fit of a single model.\n",
        "Adjusted R-squared: More appropriate when comparing the performance of multiple models with different numbers of predictors, as it accounts for model complexity and helps prevent overfitting."
      ],
      "metadata": {
        "id": "X5uKfscWHNu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Why do we use Mean Squared Error (MSE)?\n",
        "We use Mean Squared Error (MSE) primarily for these reasons:\n",
        "\n",
        "Penalizes Large Errors: Squaring the difference between the predicted and actual values amplifies the impact of larger errors. This encourages the model to minimize significant deviations, making it more sensitive to outliers.\n",
        "Differentiability: MSE is a differentiable function. This property is crucial in many machine learning algorithms, such as gradient descent, which rely on calculating the gradient of the loss function to find the optimal model parameters.\n",
        "Mathematical Convenience: MSE has convenient mathematical properties that make it easier to work with in optimization problems.\n",
        "\n",
        " For example, it has a unique minimum, which simplifies the search for the best model parameters.\n",
        "Widely Used: MSE is a widely adopted metric in various fields, including regression analysis, machine learning, and signal processing. This widespread use facilitates comparison and benchmarking of different models and techniques.\n",
        "However, it's important to note that MSE has some limitations:\n",
        "\n",
        "Sensitivity to Outliers: As mentioned earlier, the squaring operation can significantly amplify the impact of outliers, potentially skewing the evaluation towards a few extreme cases.\n",
        "May not always reflect human perception: While MSE is mathematically convenient, it may not always accurately reflect how humans perceive errors. For instance, a small error in a critical application might be more significant than a larger error in a less critical one.\n",
        "Alternatives to MSE:\n",
        "\n",
        "Mean Absolute Error (MAE): Less sensitive to outliers than MSE.\n",
        "Huber Loss: A combination of MSE and MAE, offering a balance between sensitivity to outliers and differentiability.\n",
        "The choice of error metric depends on the specific problem and the desired properties of the model.\n"
      ],
      "metadata": {
        "id": "UbShOJrvHNsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.What does an Adjusted R-squared value of 0.85 indicate?\n",
        "An Adjusted R-squared value of 0.85 indicates that 85% of the variance in the dependent variable is explained by the independent variables included in the model, while also taking into account the number of predictors and sample size"
      ],
      "metadata": {
        "id": "zjz2LdCEsNo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.How do we check for normality of residuals in linear regression?\n",
        "Methods for Checking Normality of Residuals (Short Version):\n",
        "\n",
        "Visual Inspection (Q-Q Plot): If the points in the Q-Q plot roughly form a straight line, it suggests normality. Significant deviations indicate non-normality.\n",
        "\n",
        "Statistical Tests (Shapiro-Wilk Test): If the p-value is less than your significance level (e.g., 0.05), it suggests non-normality. Otherwise, it supports normality.\n",
        "\n",
        "Histograms/Kernel Density Plots: Look for a roughly bell-shaped distribution in the histogram or kernel density plot. Skewness or kurtosis suggests non-normality."
      ],
      "metadata": {
        "id": "FWMfALyCsl1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. What is multicollinearity, and how does it impact regression?\n",
        "Multicollinearity happens when predictor variables in a regression model are highly correlated. Basically, they're giving redundant information.\n",
        "\n",
        "How does it impact regression?\n",
        "\n",
        "Makes coefficient estimates unreliable (they can change a lot with small data changes).\n",
        "Inflates standard errors, making it harder to find truly significant predictors.\n",
        "Makes it difficult to isolate the individual effect of each predictor."
      ],
      "metadata": {
        "id": "dja8HP1Bslxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.What is Mean Absolute Error (MAE)?\n",
        "In the context of machine learning and regression analysis, the Mean Absolute Error (MAE) is a metric used to measure the average absolute difference between the predicted values and the actual values in a dataset. It's a way to quantify the accuracy of a regression model's predictions.\n",
        "\n",
        "Formula\n",
        "\n",
        "Here's the formula for MAE:\n",
        "\n",
        "\n",
        "MAE = (1/n) * Î£|yi - Å·i|\n",
        "Use code with caution\n",
        "Where:\n",
        "\n",
        "n = the number of data points\n",
        "yi = the actual value of the target variable for the ith data point\n",
        "Å·i = the predicted value of the target variable for the ith data point\n",
        "Î£|yi - Å·i| = the sum of the absolute differences between actual and predicted values\n"
      ],
      "metadata": {
        "id": "4j9yTayWtZ6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.What are the benefits of using an ML pipeline?\n",
        "An ML pipeline is a sequence of steps involved in building and deploying a machine learning model. It typically includes tasks like data loading, preprocessing, feature engineering, model training, and evaluation.\n",
        "\n",
        "Benefits of Using an ML Pipeline:\n",
        "\n",
        "Reproducibility:\n",
        "\n",
        "Reasoning: Pipelines automate the entire ML workflow, ensuring that the same steps are executed in the same order every time.\n",
        "Benefit: This makes experiments more reproducible and helps to avoid errors caused by inconsistencies in the process.\n",
        "Maintainability:\n",
        "\n",
        "Reasoning: Pipelines organize code and make it more modular.\n",
        "Benefit: This makes the code easier to understand, maintain, and update. Changes can be made to individual steps without affecting the rest of the pipeline.\n",
        "Collaboration:\n",
        "\n",
        "Reasoning: Pipelines provide a clear structure for ML projects.\n",
        "Benefit: This makes it easier for teams to collaborate and share their work. Different team members can focus on specific steps in the pipeline.\n",
        "Efficiency:\n",
        "\n",
        "Reasoning: Pipelines can automate repetitive tasks, such as data cleaning and feature engineering.\n",
        "Benefit: This saves time and resources, allowing data scientists to focus on more important aspects of the project.\n",
        "Experimentation:\n",
        "\n",
        "Reasoning: Pipelines make it easier to try different models and hyperparameters.\n",
        "Benefit: This allows data scientists to quickly explore various options and find the best model for the task.\n",
        "Deployment:\n",
        "\n",
        "Reasoning: Pipelines can be used to deploy models into production environments.\n",
        "Benefit: This simplifies the deployment process and reduces the risk of errors."
      ],
      "metadata": {
        "id": "aXem_Fo5slus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Why is RMSE considered more interpretable than MSE?\n",
        "RMSE vs. MSE\n",
        "\n",
        "MSE (Mean Squared Error): Calculates the average of the squared differences between predicted and actual values. It gives more weight to larger errors due to the squaring operation.\n",
        "RMSE (Root Mean Squared Error): The square root of the MSE. It brings the error metric back to the original scale of the target variable.\n",
        "Why RMSE is More Interpretable\n",
        "\n",
        "Units of the Target Variable:\n",
        "\n",
        "RMSE is expressed in the same units as the target variable, making it easier to understand the magnitude of the error in a practical sense.\n",
        "For example, if you're predicting house prices in dollars, RMSE will also be in dollars, providing a direct measure of the average prediction error in dollars. MSE, on the other hand, would be in squared dollars, which is less intuitive.\n",
        "Relatability to the Original Scale:\n",
        "\n",
        "RMSE is more relatable because it represents the average difference between predicted and actual values on the original scale of the data. It's easier to grasp the meaning of an RMSE of 10 units (e.g., 10 dollars, 10 meters) than an MSE of 100 squared units.\n"
      ],
      "metadata": {
        "id": "IeocXdgBslrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.What is pickling in Python, and how is it useful in ML?\n",
        "Pickling is a process in Python used to serialize and deserialize Python object structures.\n",
        "\n",
        "Serialization: Converting a Python object hierarchy (like a trained machine learning model) into a byte stream. This byte stream can be stored in a file or transferred over a network.\n",
        "Deserialization: The reverse process, converting a byte stream back into a Python object hierarchy.\n",
        "How is Pickling Useful in ML?\n",
        "\n",
        "Saving Trained Models:\n",
        "\n",
        "After training a machine learning model, you can use pickling to save it to a file. This allows you to:\n",
        "Avoid retraining the model every time you need to use it.\n",
        "Share the model with others.\n",
        "Deploy the model in a production environment.\n",
        "Storing Intermediate Data Structures:\n",
        "\n",
        "Pickling can be used to save intermediate data structures generated during the machine learning process, such as:\n",
        "Feature vectors\n",
        "Data transformations\n",
        "Model hyperparameters\n",
        "Persistence:\n",
        "\n",
        "Pickling provides a mechanism to store and retrieve complex objects, enabling persistence of your machine learning workflow and data.\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train a model\n",
        "model = LinearRegression()\n",
        "# ... (train the model) ...\n",
        "\n",
        "# Save the model to a file\n",
        "with open('trained_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Later, load the model from the file\n",
        "with open('trained_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Use the loaded model for predictions\n",
        "# ..."
      ],
      "metadata": {
        "id": "H8Igxxhhslow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.What does a high R-squared value mean?\n",
        "R-squared (RÂ²) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a regression model. It is also known as the coefficient of determination.\n",
        "\n",
        "Interpretation of a High R-squared Value\n",
        "\n",
        "A high R-squared value generally indicates that the model fits the data well. More specifically:\n",
        "\n",
        "Goodness of Fit: A high R-squared suggests that the independent variables in the model are doing a good job of explaining the variability in the dependent variable. The model is able to capture the overall trend and patterns in the data.\n",
        "Predictive Power: A higher R-squared often implies better predictive power, meaning the model is likely to make more accurate predictions on new, unseen data.\n",
        "Example\n",
        "\n",
        "Let's say you're trying to predict house prices based on features like size, location, and number of bedrooms. If your model has an R-squared value of 0.90, it means that 90% of the variation in house prices can be explained by the features included in the model. This suggests that the model is a good fit for the data and can be used for reasonably accurate predictions.\n",
        "\n",
        "Important Considerations\n",
        "\n",
        "Context matters: While a high R-squared is generally desirable, the ideal value depends on the specific field and the complexity of the problem. In some fields, like social sciences, R-squared values of 0.5 or 0.6 might be considered good, while in others, like physics, values closer to 1.0 might be expected.\n",
        "Overfitting: A very high R-squared (close to 1.0) can sometimes be a sign of overfitting, where the model is too complex and has learned the noise in the data along with the underlying patterns. Overfitting can lead to poor performance on new data.\n",
        "Adjusted R-squared: The adjusted R-squared is a modified version of R-squared that takes into account the number of predictors in the model. It is often preferred when comparing models with different numbers of predictors, as it penalizes the addition of unnecessary variables."
      ],
      "metadata": {
        "id": "8Yqxn-NvslmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13.What happens if linear regression assumptions are violated?\n",
        "Consequences of Violations\n",
        "\n",
        "When these assumptions are violated, the following problems can arise:\n",
        "\n",
        "Linearity Violation:\n",
        "\n",
        "Consequence: The model may not accurately capture the true relationship between the variables, leading to biased predictions.\n",
        "Independence Violation:\n",
        "\n",
        "Consequence: Standard errors of the regression coefficients may be underestimated, leading to incorrect conclusions about the significance of the predictors.\n",
        "Normality Violation:\n",
        "\n",
        "Consequence: Confidence intervals and hypothesis tests may be unreliable, potentially leading to incorrect inferences.\n",
        "Homoscedasticity Violation (Heteroscedasticity):\n",
        "\n",
        "Consequence: Regression coefficients may be less efficient (less precise), and standard errors may be biased, affecting hypothesis tests and confidence intervals.\n",
        "Multicollinearity:\n",
        "\n",
        "Consequence: Regression coefficients become unstable and difficult to interpret, making it challenging to determine the individual effects of predictors.\n",
        "Overall Impact\n",
        "\n",
        "Violating linear regression assumptions can lead to:\n",
        "\n",
        "Biased or inefficient estimates of the regression coefficients.\n",
        "Inaccurate predictions.\n",
        "Unreliable hypothesis tests and confidence intervals.\n",
        "Misleading conclusions about the relationships between variables.\n",
        "Addressing Violations\n",
        "\n",
        "Depending on the specific assumption violated, various techniques can be used to address the issue, such as:\n",
        "\n",
        "Transforming variables (e.g., using logarithmic or square root transformations) to address non-linearity or heteroscedasticity.\n",
        "Using robust standard errors to account for heteroscedasticity or non-normality.\n",
        "Removing or combining highly correlated predictors to address multicollinearity.\n",
        "Using different regression models (e.g., generalized linear models) that are more appropriate for the data"
      ],
      "metadata": {
        "id": "N9PED6Ybsljx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14.How can we address multicollinearity in regression?\n",
        "\n",
        "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This can make it difficult to isolate the individual effects of each predictor on the dependent variable.\n",
        "\n",
        "Detecting Multicollinearity\n",
        "\n",
        "Correlation Matrix: Look for high correlations (absolute values close to 1) between independent variables.\n",
        "Variance Inflation Factor (VIF): VIF values greater than 5 or 10 generally indicate problematic multicollinearity.\n",
        "Addressing Multicollinearity\n",
        "\n",
        "Remove Highly Correlated Variables:\n",
        "\n",
        "Identify the variables with high VIF values or correlation coefficients.\n",
        "Remove one of the highly correlated variables from the model.\n",
        "Choose the variable to remove based on domain knowledge or the variable's importance in the context of your research question.\n",
        "Combine Highly Correlated Variables:\n",
        "\n",
        "If removing a variable is not desirable, consider creating a composite variable by combining the highly correlated variables.\n",
        "For example, you could create an average or a weighted average of the variables.\n",
        "Center or Standardize Variables:\n",
        "\n",
        "Centering (subtracting the mean) or standardizing (subtracting the mean and dividing by the standard deviation) predictor variables can sometimes help reduce multicollinearity, especially when interaction terms are involved.\n",
        "Principal Component Analysis (PCA):\n",
        "\n",
        "PCA is a dimensionality reduction technique that can be used to create a smaller set of uncorrelated variables (principal components) from the original set of correlated variables.\n",
        "These principal components can then be used as predictors in the regression model.\n",
        "Partial Least Squares Regression (PLSR):\n",
        "\n",
        "PLSR is another technique that can handle multicollinearity. It creates new latent variables that are linear combinations of the original predictors and are designed to maximize the covariance between the predictors and the dependent variable.\n",
        "Ridge Regression or Lasso Regression:\n",
        "\n",
        "These are regularization techniques that can help reduce the impact of multicollinearity by shrinking the regression coefficients towards zero.\n",
        "They can be particularly useful when you have a large number of predictors.\n",
        "Collect More Data:\n",
        "\n",
        "In some cases, multicollinearity can be reduced by collecting more data. A larger sample size can provide more information to distinguish between the effects of correlated variables.\n",
        "Choosing the Best Approach\n",
        "\n",
        "The best approach for addressing multicollinearity depends on the specific situation, including:\n",
        "\n",
        "Severity of multicollinearity\n",
        "Number of predictors\n",
        "Research question\n",
        "Domain knowledge\n",
        "It's often helpful to try different approaches and compare the results to determine the most effective strategy."
      ],
      "metadata": {
        "id": "GJvgLLsaslg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.How can feature selection improve model performance in regression analysis?\n",
        "Feature selection is a crucial step in regression analysis that can significantly enhance model performance. Here's how:\n",
        "\n",
        "Reduces Overfitting: By selecting only the most relevant features, the model becomes less complex and is less likely to capture noise in the training data. This helps improve the model's generalization to new, unseen data.\n",
        "\n",
        "Improves Model Interpretability: A model with fewer features is easier to interpret. It allows you to understand the impact of each predictor on the dependent variable more clearly.\n",
        "\n",
        "Enhances Computational Efficiency: Reducing the number of features decreases the computational cost of training the model. This can be especially important when dealing with large datasets or complex models.\n",
        "\n",
        "Addresses Multicollinearity: Feature selection can help identify and remove highly correlated predictors, thereby mitigating multicollinearity and improving the stability of the model's estimates.\n",
        "\n",
        "Improves Prediction Accuracy: By focusing on the most predictive features, the model can achieve better accuracy and performance. Irrelevant or redundant features can introduce noise and reduce the model's predictive power.\n",
        "\n",
        "Techniques for Feature Selection:\n",
        "\n",
        "Filter Methods: Use statistical techniques to evaluate the relationship between each feature and the target variable (e.g., correlation, chi-square test).\n",
        "\n",
        "Wrapper Methods: Use iterative techniques to select features based on model performance (e.g., forward selection, backward elimination, recursive feature elimination).\n",
        "\n",
        "Embedded Methods: Use algorithms that perform feature selection during model training (e.g., Lasso regression, decision trees)."
      ],
      "metadata": {
        "id": "Lb4EU6qzsleH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.How is Adjusted R-squared calculated?\n",
        "Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors in a regression model. It's particularly useful when comparing models with different numbers of predictors, as it penalizes the addition of unnecessary variables that don't significantly improve the model's explanatory power.\n",
        "Adjusted RÂ² = 1 - [(1 - RÂ²) * (n - 1) / (n - k - 1)]\n",
        "Where:\n",
        "\n",
        "RÂ²: The R-squared value of the model.\n",
        "n: The number of observations in the dataset.\n",
        "k: The number of predictor variables in the model.\n",
        "Breaking Down the Formula\n",
        "\n",
        "(1 - RÂ²): This represents the proportion of the variance in the dependent variable that is not explained by the model.\n",
        "(n - 1) / (n - k - 1): This is a penalty factor that adjusts for the number of predictors. It increases as the number of predictors (k) increases.\n",
        "[(1 - RÂ²) * (n - 1) / (n - k - 1)]: This represents the adjusted proportion of unexplained variance, taking into account the number of predictors.\n",
        "1 - [(1 - RÂ²) * (n - 1) / (n - k - 1)]: Finally, subtracting this adjusted proportion from 1 gives us the Adjusted R-squared.\n"
      ],
      "metadata": {
        "id": "vjGMoGIyslbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Why is MSE sensitive to outliers?\n",
        "The Squaring Effect\n",
        "\n",
        "The key reason MSE is sensitive to outliers lies in the way it's calculated:\n",
        "\n",
        "Residuals: First, the differences between predicted and actual values (residuals) are calculated.\n",
        "Squaring: Then, these residuals are squared.\n",
        "Averaging: Finally, the squared residuals are averaged to get the MSE.\n",
        "How Outliers Influence MSE\n",
        "\n",
        "Outliers have large residuals. By definition, outliers are data points that deviate significantly from the general pattern. This means they will have large residuals (the difference between their actual and predicted values).\n",
        "Squaring amplifies the effect of large residuals. When you square a large value, it becomes even larger. This means that the squared residuals of outliers contribute disproportionately to the overall MSE calculation.\n",
        "The average is pulled upwards. Since outliers have such a strong influence on the squared residuals, they can significantly increase the average (the MSE).\n",
        "Example\n",
        "\n",
        "Imagine you have a dataset of house prices. Most houses are in the $200,000 - $$200,000 - $300,000 range, but you have one outlier priced at $2,000,000. This outlier will have a very large residual, and when that residual is squared, it will have a substantial impact on the MSE, making the MSE larger than it would be without the outlier.\n",
        "\n",
        "Why this is important\n",
        "\n",
        "Model Evaluation: When evaluating a model, a high MSE might be due to a few outliers rather than a general poor fit of the model. This means you need to carefully consider the presence of outliers when interpreting the MSE.\n",
        "Model Training: Some machine learning algorithms that try to minimize MSE during training can be heavily influenced by outliers. The algorithm might adjust its parameters to fit the outliers well, potentially at the expense of the overall model's performance on the majority of the data."
      ],
      "metadata": {
        "id": "WDx8yn5oslYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.What is the role of homoscedasticity in linear regression?\n",
        "Homoscedasticity is a critical assumption in linear regression that relates to the variability of the residuals (errors). Specifically, it means that the residuals should have a constant variance at every level of the independent variable(s). Here's why homoscedasticity is important:\n",
        "\n",
        "Unbiased Estimates: When homoscedasticity holds, the estimates of the regression coefficients remain unbiased and efficient, providing reliable predictions.\n",
        "\n",
        "Valid Statistical Tests: Homoscedasticity ensures that the standard errors of the coefficients are accurate, which in turn makes hypothesis tests (like t-tests) and confidence intervals valid. This is crucial for making reliable inferences about the relationships in the data.\n",
        "\n",
        "Model Performance: Homoscedasticity contributes to the overall accuracy and performance of the regression model. When the variance of the residuals is constant, the model's predictions are more stable and trustworthy.\n",
        "\n",
        "Detecting Homoscedasticity:\n",
        "\n",
        "Visual Inspection: Plot the residuals against the fitted values or independent variables. If the spread of the residuals remains constant, homoscedasticity holds. If the spread increases or decreases, there might be heteroscedasticity.\n",
        "\n",
        "Statistical Tests: Use tests like the Breusch-Pagan test or White test to formally assess homoscedasticity.\n",
        "\n",
        "Addressing Heteroscedasticity:\n",
        "\n",
        "Transformation: Transforming the dependent variable (e.g., log transformation) can stabilize the variance.\n",
        "\n",
        "Weighted Least Squares: Using weighted least squares regression gives different weights to observations based on their variance, addressing heteroscedasticity.\n",
        "\n",
        "Robust Standard Errors: Calculating robust standard errors can adjust for heteroscedasticity, providing more reliable hypothesis tests and confidence intervals.\n",
        "\n",
        "Maintaining homoscedasticity ensures that your linear regression model provides accurate and meaningful insights. If you have any more questions or need further assistance, feel free to ask!\n",
        "\n"
      ],
      "metadata": {
        "id": "PFBSkramslV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.What is Root Mean Squared Error (RMSE)?\n",
        "RMSE stands for Root Mean Squared Error. It is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.\n",
        "\n",
        "How is it calculated?\n",
        "\n",
        "Calculate the residuals: Residuals are the differences between the predicted values and the actual values.\n",
        "Square the residuals: This step emphasizes larger errors and makes all values positive.\n",
        "Calculate the mean of the squared residuals: This gives you the Mean Squared Error (MSE).\n",
        "Take the square root of the MSE: This gives you the RMSE.\n",
        "Formula:\n",
        "\n",
        "\n",
        "RMSE = â[ Î£(yi â Å·i)Â² / n ]\n",
        "Use code with caution\n",
        "Where:\n",
        "\n",
        "yi: The actual value for the ith observation.\n",
        "Å·i: The predicted value for the ith observation.\n",
        "n: The total number of observations.\n",
        "Î£: The summation symbol (summing over all observations).\n"
      ],
      "metadata": {
        "id": "3BnBSHShslTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Why is pickling considered risky?\n",
        "The primary concern with pickling is its potential security vulnerabilities.\n",
        "\n",
        "Arbitrary Code Execution:\n",
        "\n",
        "When you unpickle data, Python essentially executes the code contained within the pickle file to reconstruct the objects. This means if a pickle file comes from an untrusted source, it could contain malicious code that could be executed on your system.\n",
        "Attackers can craft pickle files that, when unpickled, trigger unintended actions, such as deleting files, accessing sensitive information, or even taking control of your system.\n",
        "Data Injection:\n",
        "\n",
        "Pickling can also be vulnerable to data injection attacks, where attackers modify the pickled data to inject malicious code or alter the behavior of your program.\n",
        "Other Risks\n",
        "\n",
        "Besides security, there are other potential risks associated with pickling:\n",
        "\n",
        "Compatibility Issues:\n",
        "\n",
        "Pickle files might not be compatible across different Python versions or environments due to changes in object structures or library dependencies. This can lead to errors when trying to load pickled models or data in a different setup.\n",
        "Maintenance Challenges:\n",
        "\n",
        "Pickling relies on the internal structure of objects, which can evolve over time as libraries are updated. This can break the unpickling process if a model or data was saved with an older version of a library and loaded with a newer one.\n",
        "Data Corruption:\n",
        "\n",
        "If a pickle file gets corrupted during storage or transmission, it might become unreadable or lead to unexpected errors when unpickled.\n",
        "Mitigation Strategies\n",
        "\n",
        "To mitigate the risks associated with pickling:\n",
        "\n",
        "Only unpickle data from trusted sources. Avoid loading pickle files from unknown or potentially malicious websites or email attachments.\n",
        "Consider using alternatives like Joblib, ONNX, or PMML for saving and loading models, especially in production environments, where security and compatibility are critical.\n",
        "When using pickle, use a secure protocol like pickle.HIGHEST_PROTOCOL to minimize the risk of code injection.\n",
        "Regularly update your Python environment and libraries to reduce compatibility issues.\n",
        "Implement error handling mechanisms to gracefully handle potential exceptions during the unpickling process."
      ],
      "metadata": {
        "id": "LP7c6VNtslQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.What alternatives exist to pickling for saving ML models?\n",
        "Here are a few popular options for saving your trained machine learning models besides pickling:\n",
        "\n",
        "Joblib:\n",
        "\n",
        "Often preferred for scikit-learn models, as it's more efficient with NumPy arrays (common in ML).\n",
        "Handles large data better than pickle.\n",
        "Similar API (dump/load), making it easy to switch.\n",
        "ONNX (Open Neural Network Exchange):\n",
        "\n",
        "Great for deep learning models and interoperability between frameworks (PyTorch, TensorFlow, etc.).\n",
        "Open format for wider compatibility.\n",
        "Enables sharing models across different platforms.\n",
        "PMML (Predictive Model Markup Language):\n",
        "\n",
        "XML-based, focusing on model structure rather than implementation.\n",
        "Widely supported for deployment in various environments.\n",
        "Useful when sharing with systems that don't use Python.\n",
        "Native Model Persistence:\n",
        "\n",
        "Many libraries (like TensorFlow, PyTorch) have their own methods for saving and loading models.\n",
        "Often optimized for their specific model types.\n",
        "Consider this if your library offers it."
      ],
      "metadata": {
        "id": "8Bc7fbv2slLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.What is heteroscedasticity, and why is it a problem?\n",
        "Okay, let's discuss heteroscedasticity and why it poses a problem in regression analysis:\n",
        "\n",
        "What is Heteroscedasticity?\n",
        "\n",
        "In regression analysis, heteroscedasticity refers to a situation where the variability of the residuals (the differences between observed and predicted values) is not constant across the range of predictor values.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "Imagine you're plotting your data and the regression line. If the scatter of the data points around the line is uneven â wider in some areas and narrower in others â you likely have heteroscedasticity. This means the error variance is not consistent across your data.\n",
        "\n",
        "Why is Heteroscedasticity a Problem?\n",
        "\n",
        "Inefficient Estimates: While heteroscedasticity doesn't bias the coefficient estimates (the values of your model's parameters), it makes them less precise. This means your model might not be as accurate as it could be if the errors were homoscedastic (having constant variance).\n",
        "Unreliable Hypothesis Tests: The standard errors of the coefficients are affected by heteroscedasticity. This leads to incorrect p-values and confidence intervals, potentially leading you to draw wrong conclusions about the significance of your predictors.\n",
        "Misleading Predictions: Heteroscedasticity can undermine the reliability of prediction intervals. Your model might be more accurate for some ranges of predictor values and less accurate for others, making its predictions less trustworthy overall.\n",
        "Example\n",
        "\n",
        "Consider predicting income based on years of experience. Early in a career, income might be more variable due to factors like entry-level jobs or rapid promotions. As experience increases, income often becomes more stable. This uneven variability in income (the dependent variable) across experience levels (the predictor) demonstrates heteroscedasticity.\n",
        "\n",
        "Detecting Heteroscedasticity\n",
        "\n",
        "Residual plots: Plotting residuals against predicted values can visually reveal patterns of heteroscedasticity (e.g., a cone-shaped pattern).\n",
        "Statistical tests: Tests like the Breusch-Pagan test or White test can formally assess the presence of heteroscedasticity.\n",
        "Addressing Heteroscedasticity\n",
        "\n",
        "Transformations: Applying transformations to the dependent or independent variables (e.g., log transformation) can sometimes stabilize the variance.\n",
        "Weighted least squares: This technique gives more weight to observations with smaller variances, making the estimates more efficient.\n",
        "Robust standard errors: Using robust standard errors can help to adjust for heteroscedasticity and provide more accurate hypothesis tests\n"
      ],
      "metadata": {
        "id": "DFUxbsbFslFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.How can interaction terms enhance a regression model's predictive power?\n",
        "Interaction terms boost predictions by:\n",
        "\n",
        "Capturing synergy: They model how the effect of one predictor changes based on the value of another. Like how price sensitivity might depend on product quality.\n",
        "Flexibility: They let the model fit the data more closely, leading to better accuracy.\n",
        "Unveiling complexity: They can reveal hidden relationships, such as how a drug's effectiveness varies with age.\n",
        "Think of it like this: Imagine predicting crop yield based on rainfall and fertilizer. An interaction term would account for the fact that fertilizer is more effective when there's enough rain. It's about understanding the combined effects, not just individual contributions.\n",
        "\n",
        "In short, interaction terms help models be more nuanced and realistic, leading to more accurate predictions."
      ],
      "metadata": {
        "id": "-gJjUQsyslCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical questions:\n",
        "#1.Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model\n",
        "#using Seaborn's \"diamonds\" dataset\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table']]  # Independent variables\n",
        "y = diamonds['price']  # Dependent variable\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals (errors)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Visualize the distribution of residuals using Seaborn\n",
        "sns.displot(residuals, kind='kde')  # Kernel Density Estimation plot\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Create a histogram of residuals\n",
        "# sns.histplot(residuals, bins=30)\n",
        "# plt.title('Histogram of Residuals')\n",
        "# plt.xlabel('Residuals')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "OTuqviXcHNqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root\n",
        "#Mean Squared Error (RMSE) for a linear regression model\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 5, 4, 6])\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "TmTJ6IQFnWtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check\n",
        "#linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset (replace with your actual data)\n",
        "# For this example, we'll use the built-in 'tips' dataset\n",
        "data = sns.load_dataset('tips')\n",
        "\n",
        "# Define features and target\n",
        "X = data[['total_bill', 'size']]\n",
        "y = data['tip']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Check for Linearity: Scatter plot of predicted vs. actual values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Predicted vs. Actual Values\")\n",
        "plt.show()\n",
        "\n",
        "# Check for Homoscedasticity: Residuals plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Predicted Values\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\n",
        "plt.show()\n",
        "\n",
        "# Check for Multicollinearity: Correlation matrix\n",
        "correlation_matrix = X_train.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dhxz1_c8nwS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the\n",
        "#performance of different regression models\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns  # Import seaborn for loading the 'tips' dataset\n",
        "\n",
        "# Load your dataset (replace with your actual data)\n",
        "# For this example, we'll use the built-in 'tips' dataset\n",
        "data = sns.load_dataset('tips')\n",
        "\n",
        "# Define features and target\n",
        "X = data[['total_bill', 'size']]\n",
        "y = data['tip']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to evaluate models\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a regression model.\n",
        "\n",
        "    Args:\n",
        "        model: The regression model to evaluate.\n",
        "        X_train: Training features.\n",
        "        y_train: Training target.\n",
        "        X_test: Testing features.\n",
        "        y_test: Testing target.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the model name, MSE, and R-squared score.\n",
        "    \"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return {'model': type(model).__name__, 'MSE': mse, 'R-squared': r2}\n",
        "\n",
        "# Define pipelines for each model with feature scaling\n",
        "pipeline_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline_rf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_svr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', SVR())\n",
        "])\n",
        "\n",
        "# Evaluate each model\n",
        "models = [pipeline_lr, pipeline_rf, pipeline_svr]\n",
        "results = []\n",
        "for model in models:\n",
        "    results.append(evaluate_model(model, X_train, y_train, X_test, y_test))\n",
        "\n",
        "# Print results\n",
        "for result in results:\n",
        "    print(f\"{result['model']}:\")\n",
        "    print(f\"  MSE: {result['MSE']:.4f}\")\n",
        "    print(f\"  R-squared: {result['R-squared']:.4f}\")\n",
        "    print(\"-\" * 20)\n"
      ],
      "metadata": {
        "id": "eXvs4qagSw-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and\n",
        "#R-squared score.\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Define the feature (X) and target (y)\n",
        "X = tips['total_bill'].values.reshape(-1, 1)  # Reshape to a 2D array\n",
        "y = tips['tip']\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get the model's coefficients and intercept\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate the R-squared score\n",
        "r_squared = r2_score(y, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"R-squared:\", r_squared)"
      ],
      "metadata": {
        "id": "Jl-TjmLwHNoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using\n",
        "#simple linear regression and visualizes the results\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Create a scatter plot to visualize the relationship\n",
        "# between total bill and tip\n",
        "sns.scatterplot(x='total_bill', y='tip', data=tips)\n",
        "plt.title('Relationship between Total Bill and Tip')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.show()\n",
        "\n",
        "# Define the feature (X) and target (y)\n",
        "X = tips['total_bill'].values.reshape(-1, 1)\n",
        "y = tips['tip']\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get the model's coefficients and intercept\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Print the equation of the regression line\n",
        "print(f\"Regression line equation: y = {coefficients[0]:.2f}x + {intercept:.2f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the regression line on the scatter plot\n",
        "plt.scatter(X, y, color='blue', label='Actual Data')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line')\n",
        "plt.title('Linear Regression: Total Bill vs. Tip')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fV16oT1rPmZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the\n",
        "#model to predict new values and plot the data points along with the regression line\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, 1 feature (between 0 and 10)\n",
        "y = 2 * X + 3 + np.random.randn(100, 1) * 2  # Target variable with noise\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the equation of the regression line\n",
        "#print(f\"Regression line equation: y = {coefficients[0]:.2f}x + {intercept:.2f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the regression line on the scatter plot\n",
        "plt.scatter(X, y, color='blue', label='Actual Data')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line')\n",
        "plt.title('Linear Regression with synthetic data')\n",
        "plt.xlabel('Feature(x)')\n",
        "plt.ylabel('Feature(y)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ltbjcm3ANGbH",
        "outputId": "d28bb07d-14be-4f40-ddda-726f6337e35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python script that pickles a trained linear regression model and saves it to a file.\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save the model to a file using pickle\n",
        "filename = 'linear_regression_model.pkl'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"Model saved to {filename}\")"
      ],
      "metadata": {
        "id": "L1AmPXWYNGfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the\n",
        "#regression curve.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, 1 feature (between 0 and 10)\n",
        "y = 2 * X**2 + 3 * X + 5 + np.random.randn(100, 1) * 10  # Target variable with noise\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Generate points for plotting the curve\n",
        "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_plot_poly = poly_features.transform(X_plot)\n",
        "y_plot = model.predict(X_plot_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_plot, y_plot, color='red', label='Regression Curve')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L6oYq9uSNGhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear\n",
        "#regression model to the data. Print the model's coefficient and intercept.\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, 1 feature (between 0 and 10)\n",
        "y = 2 * X + 3 + np.random.randn(100, 1) * 2  # Target variable with noise\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the model's coefficient and intercept\n",
        "print(\"Coefficient:\", model.coef_[0])  # Access the coefficient (slope)\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "id": "-q5MoM_vNGkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and\n",
        "#compares their performance.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, 1 feature (between 0 and 10)\n",
        "y = 2 * X**3 - 5 * X**2 + 3 * X + 5 + np.random.randn(100, 1) * 10  # Target variable with noise\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define degrees of polynomial regression to try\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Loop through different degrees and fit models\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly_features.fit_transform(X_train)\n",
        "    X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "\n",
        "    # Calculate R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Degree {degree}: R-squared = {r2:.4f}\")\n",
        "\n",
        "    # Visualize the regression curve\n",
        "    X_plot = np.linspace(0"
      ],
      "metadata": {
        "id": "_gRr8AzoNGm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python script that fits a simple linear regression model with two features and prints the model's\n",
        "#coefficients, intercept, and R-squared score.\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Sample data with two features\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([3, 7, 11, 15, 19])\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get model parameters\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "ehZOO-b2ol0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the\n",
        "#regression line along with the data points.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = np.random.rand(100, 1)  # 100 samples with 1 feature\n",
        "y = 2 * X.flatten() + 1 + 0.1 * np.random.randn(100)  # Generate target values with noise\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data and regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, label=\"Data Points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Regression Line\")\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DoYRl6-colrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset\n",
        "#with multiple features.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a sample dataset with multiple features\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "def calculate_vif(df):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data['feature'] = df.columns\n",
        "    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "    return vif_data\n",
        "\n",
        "# Display VIF results\n",
        "vif_results = calculate_vif(df)\n",
        "print(vif_results)\n"
      ],
      "metadata": {
        "id": "oC-QXQSDNGrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a\n",
        "#polynomial regression model, and plots the regression curve\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, 1 feature (between 0 and 10)\n",
        "y = 0.5 * X**4 - 2 * X**3 + 3 * X**2 + 2 * X + 5 + np.random.randn(100, 1) * 10  # Target variable with noise\n",
        "\n",
        "# Create polynomial features (degree 4)\n",
        "poly_features = PolynomialFeatures(degree=4, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Generate points for plotting the curve\n",
        "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_plot_poly = poly_features.transform(X_plot)\n",
        "y_plot = model.predict(X_plot_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_plot, y_plot, color='red', label='Regression Curve')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aooI_xnsNGuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python script that creates a machine learning pipeline with data standardization and a multiple\n",
        "#linear regression model, and prints the R-squared score.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your dataset (replace with your data loading code)\n",
        "# Assuming your data is in a CSV file named 'data.csv'\n",
        "data = pd.read_csv('data.csv')\n",
        "X = data[['feature1', 'feature2', 'feature3']]  # Replace with your feature columns\n",
        "y = data['target']  # Replace with your target column\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize features\n",
        "    ('regressor', LinearRegression())  # Multiple linear regression model\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "MKciXB6cNGxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the\n",
        "#regression curve.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, 1 feature (between 0 and 10)\n",
        "y = 2 * X**3 - 5 * X**2 + 3 * X + 5 + np.random.randn(100, 1) * 10  # Target variable with noise\n",
        "\n",
        "# Create polynomial features (degree 3)\n",
        "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Generate points for plotting the curve\n",
        "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_plot_poly = poly_features.transform(X_plot)\n",
        "y_plot = model.predict(X_plot_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_plot, y_plot, color='red', label='Regression Curve')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uI3nKkFANGzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print\n",
        "#the R-squared score and model coefficients\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data with 5 features\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
        "y = 2*X[:,0] + 3*X[:,1] - X[:,2] + 0.5*X[:,3] + 2*X[:,4] + 5 + np.random.randn(100)  # Target variable\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "id": "TmPirEk8NG1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the\n",
        "#data points along with the regression line.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Visualize the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label='Regression line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Linear Regression')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "dmqGEM52NG36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R-squared score and coefficients\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with 3 features\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples, 3)  # 3 features\n",
        "true_coefs = np.array([2.5, -1.0, 0.7])  # True coefficients\n",
        "noise = np.random.randn(n_samples) * 0.5  # Add some noise\n",
        "y = np.dot(X, true_coefs) + noise\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"R-squared score:\", r2)\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "HSnXPiSJNG6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python script that demonstrates how to serialize and deserialize machine learning models using\n",
        "#joblib instead of pickling.\n",
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create and train a Linear Regression model (for demonstration)\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Serialize the model using joblib\n",
        "joblib.dump(model, 'linear_regression_model.joblib')\n",
        "\n",
        "# Deserialize the model from the file\n",
        "loaded_model = joblib.load('linear_regression_model.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = loaded_model.predict(X)\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "dQve8JrVNG9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use\n",
        "#the Seaborn 'tips' dataset\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the tips dataset\n",
        "tips_df = sns.load_dataset('tips')\n",
        "\n",
        "# Select features and target variable\n",
        "X = tips_df[['total_bill', 'sex', 'smoker', 'day', 'time']]\n",
        "y = tips_df['tip']\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(X, columns=['sex', 'smoker', 'day', 'time'], drop_first=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.2f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Coefficients:\")\n",
        "for feature, coef in zip(X_train.columns, model.coef_):\n",
        "    print(f\"{feature}: {coef:.2f}\")"
      ],
      "metadata": {
        "id": "qlgNaStiNHAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R squared score\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "n_features = 5\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "true_coefs = np.random.randn(n_features)\n",
        "noise = np.random.randn(n_samples) * 0.5\n",
        "y = np.dot(X, true_coefs) + noise\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_y_pred = lr_model.predict(X_test)\n",
        "lr_r2 = r2_score(y_test, lr_y_pred)\n",
        "lr_coefs = lr_model.coef_\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0)  # Adjust alpha as needed\n",
        "ridge_model.fit(X_train, y_train)\n",
        "ridge_y_pred = ridge_model.predict(X_test)\n",
        "ridge_r2 = r2_score(y_test, ridge_y_pred)\n",
        "ridge_coefs = ridge_model.coef_\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Regression:\")\n",
        "print(\"  R-squared:\", lr_r2)\n",
        "print(\"  Coefficients:\", lr_coefs)\n",
        "print(\"\\nRidge Regression (alpha=1.0):\")\n",
        "print(\"  R-squared:\", ridge_r2)\n",
        "print(\"  Coefficients:\", ridge_coefs)"
      ],
      "metadata": {
        "id": "2Uld7x1OkF2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic\n",
        "#dataset\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "n_features = 5\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "true_coefs = np.random.randn(n_features)\n",
        "noise = np.random.randn(n_samples) * 0.5\n",
        "y = np.dot(X, true_coefs) + noise\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation R-squared scores:\", scores)\n",
        "print(\"Mean R-squared:\", scores.mean())\n",
        "print(\"Standard deviation of R-squared:\", scores.std())"
      ],
      "metadata": {
        "id": "W45EcZGNkF5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python script that compares polynomial regression models of different degrees and prints the R squared score for each.\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def polynomial_regression(X, y, degrees):\n",
        "    \"\"\"\n",
        "    Compares polynomial regression models of different degrees.\n",
        "\n",
        "    Args:\n",
        "        X: Input features (1D array).\n",
        "        y: Target variable.\n",
        "        degrees: List of polynomial degrees to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing R-squared scores for each degree.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {}\n",
        "    for degree in degrees:\n",
        "        # Create polynomial features\n",
        "        poly = PolynomialFeatures(degree=degree)\n",
        "        X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_poly, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Create and train the model\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate R-squared score\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        results[degree] = r2\n",
        "\n",
        "    return results\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)  # 1D array\n",
        "y = X**2 + np.random.randn(100) * 5  # Quadratic relationship with noise\n",
        "\n",
        "# Define degrees to evaluate\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Run polynomial regression\n",
        "r2_scores = polynomial_regression(X, y, degrees)\n",
        "\n",
        "# Print results\n",
        "for degree, r2 in r2_scores.items():\n",
        "    print(f\"Degree {degree}: R-squared = {r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "HtbioDDOkF7z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}