{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9g00kiZ4cSkRk+7NI8mTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUHUPAGARWAL1515/Python-codes/blob/main/Logistic_regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1.What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        "Logistic Regression: A classification algorithm used to predict the probability of a categorical outcome (e.g., yes/no, 0/1, spam/not spam). It outputs a probability between 0 and 1.\n",
        "Linear Regression: A regression algorithm used to predict a continuous outcome (e.g., house prices, temperature). It outputs a real value.\n",
        "Key Difference: Logistic Regression uses a sigmoid function to squash the output of a linear combination of features into a probability. Linear Regression directly outputs a linear combination of features.\n",
        "\n"
      ],
      "metadata": {
        "id": "37mAA3W-Vh_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2.What is the mathematical equation of Logistic Regression.\n",
        "p(y=1|x) = σ(z)\n",
        "z = wTx + b\n",
        "\n",
        "p(y=1|x): Probability of the outcome y being 1 given input features x.\n",
        "σ(z): Sigmoid function: 1 / (1 + exp(-z))\n",
        "w: Vector of coefficients (weights) for the features.\n",
        "x: Vector of input features.\n",
        "b: Bias (intercept) term."
      ],
      "metadata": {
        "id": "PYJRhCkdVh88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.Why do we use the Sigmoid function in Logistic Regression.\n",
        "Probability Output: The sigmoid function maps any real number to a value between 0 and 1, which can be interpreted as a probability.\n",
        "Non-linearity: It introduces non-linearity, allowing the model to learn complex relationships between features and outcomes."
      ],
      "metadata": {
        "id": "Ji2l31R4Vh6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.What is the cost function of Logistic Regression?\n",
        "\n",
        "The cost function is typically binary cross-entropy loss (also called log loss):\n",
        "\n",
        "J(w, b) = -[y log(p(y=1|x)) + (1-y) log(1 - p(y=1|x))]\n",
        "y: True label (0 or 1).\n",
        "p(y=1|x): Predicted probability.\n",
        "This cost function penalizes the model more when it's very confident about an incorrect prediction."
      ],
      "metadata": {
        "id": "6-LM0xUSVh4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Regularization: A technique to prevent overfitting by adding a penalty term to the cost function.\n",
        "Why it's needed: Overfitting occurs when the model learns the training data too well, including noise, and doesn't generalize well to new data. Regularization discourages the model from having very large coefficients."
      ],
      "metadata": {
        "id": "BpGv1u-3Vh2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "The Core Concept: Regularization\n",
        "\n",
        "Before diving into the specifics, remember that regularization adds a penalty term to the cost function of a linear regression model. This penalty discourages the model from assigning excessively large coefficients to features, which is a common symptom of overfitting.\n",
        "\n",
        "1. Ridge Regression (L2 Regularization)\n",
        "\n",
        "Penalty Term: Adds the squared magnitude of the coefficients to the cost function.\n",
        "Cost Function: Loss + λ * Σ(β_i^2)\n",
        "Where:\n",
        "Loss is the original cost function (e.g., Mean Squared Error).\n",
        "λ (lambda) is the regularization parameter, controlling the strength of the penalty.\n",
        "β_i are the coefficients of the features.\n",
        "\n",
        "Effect:\n",
        "Shrinks the coefficients towards zero, but rarely makes them exactly zero.\n",
        "Reduces the impact of less important features.\n",
        "Effective in reducing multicollinearity (high correlation between features).\n",
        "When to Use:\n",
        "When you have many features, and you want to reduce their impact without eliminating them entirely.\n",
        "When multicollinearity is a concern.\n",
        "2. Lasso Regression (L1 Regularization)\n",
        "\n",
        "Penalty Term: Adds the absolute magnitude of the coefficients to the cost function.\n",
        "Cost Function: Loss + λ * Σ(|β_i|)\n",
        "\n",
        "Effect:\n",
        "Shrinks some coefficients to exactly zero, effectively performing feature selection.\n",
        "Creates a simpler, more interpretable model by eliminating irrelevant features.\n",
        "When to Use:\n",
        "When you suspect that many features are irrelevant.\n",
        "When you want to perform feature selection automatically.\n",
        "When you want a sparse model (a model with few non-zero coefficients).\n",
        "3. Elastic Net Regression\n",
        "\n",
        "Penalty Term: Combines both L1 and L2 regularization.\n",
        "Cost Function: Loss + λ1 * Σ(|β_i|) + λ2 * Σ(β_i^2)\n",
        "Where:\n",
        "λ1 and λ2 are regularization parameters controlling the strength of L1 and L2 penalties, respectively.\n",
        "\n",
        "Effect:\n",
        "Provides a balance between feature selection (Lasso) and coefficient shrinkage (Ridge).\n",
        "Can handle situations where there are highly correlated features.\n",
        "When to Use:\n",
        "When you have many features, some of which are correlated.\n",
        "When you want the benefits of both Lasso and Ridge.\n",
        "When Lasso might select one of a group of highly correlated variables arbitrarily, Elastic Net is more stable.\n",
        "Key Differences Summarized:\n",
        "\n",
        "Penalty Type:\n",
        "Ridge: L2 (squared magnitude)\n",
        "Lasso: L1 (absolute magnitude)\n",
        "Elastic Net: Combination of L1 and L2\n",
        "Feature Selection:\n",
        "Ridge: No (coefficients shrink towards zero)\n",
        "Lasso: Yes (coefficients can be exactly zero)\n",
        "Elastic Net: Yes (to varying degrees, depending on the parameters)\n",
        "Multicollinearity:\n",
        "Ridge: Handles well\n",
        "Lasso: Can have issues\n",
        "Elastic Net: Handles well\n",
        "In essence:\n",
        "\n",
        "If you want to reduce the impact of less important features and handle multicollinearity, use Ridge.\n",
        "If you want to perform feature selection and create a simpler model, use Lasso.\n",
        "If you want a balance of both, or have correlated features, use Elastic Net."
      ],
      "metadata": {
        "id": "8CHyGfnI82MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Use Elastic Net when you have a large number of features and suspect that some of them might be correlated. It provides a balance between feature selection (Lasso) and coefficient shrinkage (Ridge), handling multicollinearity better than Lasso."
      ],
      "metadata": {
        "id": "XjC0r9eSVhyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Larger λ: Stronger regularization. Coefficients are shrunk more aggressively towards zero, potentially leading to underfitting if λ is too large.\n",
        "Smaller λ: Weaker regularization. The model is more prone to overfitting"
      ],
      "metadata": {
        "id": "pkHa56JmVhv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Linearity between features and log-odds: The logit (log-odds) of the outcome is assumed to be a linear combination of the features.\n",
        "Independent errors: Errors are assumed to be independent.\n",
        "No or little multicollinearity: Features should not be highly correlated with each other.\n",
        "Sufficiently large sample size: Especially important when dealing with many features."
      ],
      "metadata": {
        "id": "qW28fyy8Vhr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "Decision Trees\n",
        "Random Forests\n",
        "Naive Bayes\n",
        "K-Nearest Neighbors (KNN)\n",
        "Neural Networks"
      ],
      "metadata": {
        "id": "xg1OahRmVhJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.What are Classification Evaluation Metrics?\n",
        "\n",
        "Accuracy: Overall correctness of predictions.\n",
        "Precision: Proportion of true positives among predicted positives.\n",
        "Recall (Sensitivity): Proportion of true positives among actual positives.\n",
        "F1-score: Harmonic mean of precision and recall.\n",
        "ROC-AUC: Area under the Receiver Operating Characteristic curve, measuring the ability to distinguish between classes.\n",
        "Confusion Matrix: A table summarizing the performance of a classification model."
      ],
      "metadata": {
        "id": "F5Nnnu21XJ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12.How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Class imbalance (when one class has significantly more samples than another) can bias the model towards the majority class. It might predict the majority class more often, even when it's incorrect for the minority class"
      ],
      "metadata": {
        "id": "7WFH6UFYXJ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " 13.What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Hyperparameters (e.g., C, penalty, solver) are parameters that are not learned during training. Hyperparameter tuning involves finding the optimal combination of these parameters to improve model performance. Techniques include Grid Search, Randomized Search, and Bayesian Optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "PiV1TMN3XJ4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14.What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Solvers: Algorithms used to optimize the cost function. Examples: 'liblinear', 'saga', 'lbfgs', 'newton-cg', 'sag'.\n",
        "Choice:\n",
        "'liblinear': Good for small datasets and L1 or L2 regularization.\n",
        "'saga': Handles large datasets and L1 regularization well.\n",
        "'lbfgs', 'newton-cg': Suitable for larger datasets, multiclass problems, and no regularization or L2 regularization."
      ],
      "metadata": {
        "id": "iK8sGNVQXJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "One-vs-Rest (OvR): Train a separate logistic regression classifier for each class against the rest.\n",
        "Softmax Regression: A generalization of logistic regression that directly models the probabilities of all classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "_TPQArJpXR01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "Logistic Regression, while a powerful and widely used algorithm, comes with its own set of strengths and weaknesses. Here's a breakdown:\n",
        "\n",
        "Advantages of Logistic Regression:\n",
        "\n",
        "Simplicity and Interpretability:\n",
        "Logistic Regression is relatively easy to understand and implement.\n",
        "The coefficients of the model can be interpreted to understand the relationship between the features and the outcome (in terms of log-odds).\n",
        "Efficiency:\n",
        "It's computationally efficient, especially for smaller datasets.\n",
        "Training is generally fast compared to more complex algorithms.\n",
        "Probability Outputs:\n",
        "It provides probability estimates for the predicted outcomes, which can be useful in various applications.\n",
        "This allows for setting different thresholds depending on the application.\n",
        "Regularization:\n",
        "It can be regularized (L1, L2, Elastic Net) to prevent overfitting, improving its generalization performance.\n",
        "Works well with linearly separable data:\n",
        "When the data can be separated by a linear decision boundary, logistic regression can perform very well.\n",
        "Disadvantages of Logistic Regression:\n",
        "\n",
        "Linearity Assumption:\n",
        "It assumes a linear relationship between the features and the log-odds of the outcome. If this assumption is violated, the model's performance may suffer.\n",
        "Sensitivity to Multicollinearity:\n",
        "High correlation between features (multicollinearity) can affect the stability and interpretability of the coefficients.\n",
        "Limited Complexity:\n",
        "It's not well-suited for capturing complex, non-linear relationships in the data.\n",
        "For highly non-linear data, other algorithms like decision trees or neural networks may be more appropriate.\n",
        "Outliers:\n",
        "Sensitive to outliers. Outliers can greatly influence the model's coefficients.\n",
        "Difficulty with complex relationships:\n",
        "It can struggle to model complex interactions between features"
      ],
      "metadata": {
        "id": "W25hH_5x9Yox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17.What are some use cases of Logistic Regression?\n",
        "\n",
        "Spam detection\n",
        "Medical diagnosis\n",
        "Customer churn prediction\n",
        "Credit risk assessment\n",
        "Image classification (simple cases)\n"
      ],
      "metadata": {
        "id": "zlwipvz6XRyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18.What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Logistic Regression: Used for binary classification (two classes).\n",
        "Softmax Regression: Used for multiclass classification (more than two classes). Softmax outputs a probability distribution over all classes.\n"
      ],
      "metadata": {
        "id": "oK54o8fyYEs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " 19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "OvR: Simpler to implement, can be more efficient for a large number of classes.\n",
        "Softmax: More statistically sound, as it models the probabilities of all classes simultaneously. Often preferred when the classes are mutually exclusive."
      ],
      "metadata": {
        "id": "2jef_2OJXRvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20.How do we interpret coefficients in Logistic Regression?\n",
        "Interpreting coefficients in Logistic Regression requires understanding that they represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, holding all other variables constant.  Let's break it down:\n",
        "\n",
        "1. Log-Odds (Logit):\n",
        "\n",
        "Odds: The odds of an event occurring is the ratio of the probability of the event occurring to the probability of it not occurring. odds = p(event) / (1 - p(event))\n",
        "Log-Odds (Logit): The log of the odds. log-odds = log(odds) = log(p(event) / (1 - p(event)))\n",
        "Logistic Regression models the log-odds as a linear combination of the predictor variables.\n",
        "\n",
        "2. Interpreting the Coefficients:\n",
        "\n",
        "The coefficient for a predictor variable (let's call it 'x') tells you how much the log-odds of the outcome change for a one-unit increase in 'x', assuming all other variables in the model are held constant.\n",
        "\n",
        "Positive Coefficient: A positive coefficient means that as 'x' increases, the log-odds of the outcome increase. This translates to an increase in the probability of the outcome occurring.\n",
        "Negative Coefficient: A negative coefficient means that as 'x' increases, the log-odds of the outcome decrease. This translates to a decrease in the probability of the outcome occurring.\n",
        "Magnitude of the Coefficient: The larger the absolute value of the coefficient, the greater the impact of 'x' on the log-odds (and thus, the probability).\n",
        "3. Converting to Probabilities (Optional):\n",
        "\n",
        "While the coefficients are in log-odds, you can convert them to probabilities to make the interpretation more intuitive.  However, the direct interpretation is in terms of log-odds.\n",
        "\n",
        "Here's how you can get an approximate sense of the change in probability (it's not exact because the relationship is non-linear):\n",
        "\n",
        "Calculate the change in log-odds: This is simply the coefficient itself.\n",
        "Exponentiate the change in log-odds: exp(coefficient). This gives you the odds ratio.\n",
        "Interpret the odds ratio:\n",
        "An odds ratio greater than 1 means the odds of the outcome are increased for a one-unit increase in the predictor.\n",
        "An odds ratio less than 1 means the odds of the outcome are decreased.\n",
        "Example:\n",
        "\n",
        "Suppose you are modeling the probability of a customer clicking on an ad (outcome = 1) based on the number of times they visited the website (predictor 'visits').  The coefficient for 'visits' is 0.5.\n",
        "\n",
        "Log-odds interpretation: For each additional visit to the website, the log-odds of the customer clicking on the ad increase by 0.5, holding other variables constant.\n",
        "Odds ratio interpretation: exp(0.5) ≈ 1.65. For each additional visit, the odds of clicking on the ad are approximately 1.65 times greater.\n",
        "Important Considerations:\n",
        "\n",
        "Ceteris Paribus: The interpretation of coefficients always assumes ceteris paribus (all other things being equal). It's crucial to remember that this is a statistical model and real-world scenarios might be more complex.\n",
        "Scaling: If your predictor variables are on different scales, you might want to standardize them (mean 0, standard deviation 1) before running Logistic Regression. This makes the coefficients more directly comparable.\n",
        "Interaction Terms: If you have interaction terms in your model, the interpretation of the coefficients becomes more nuanced. The effect of one variable might depend on the value of another.\n",
        "Non-Linearity: Remember that the relationship between the predictor and the probability of the outcome is non-linear (due to the sigmoid function). The coefficient represents the change in log-odds, not a direct change in probability"
      ],
      "metadata": {
        "id": "_rEbAGueXRtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pratical_Questions"
      ],
      "metadata": {
        "id": "GdW6qSzqXCJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAUscWBoQg7j"
      },
      "outputs": [],
      "source": [
        "#1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "#Regression, and prints the model accuracy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification  # For sample data\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "#and print the model accuracy\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Apply Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')  # Note: liblinear supports L1\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(\"L1 Regularization Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "ERhpuOm1QihP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "#LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Apply Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2')  # Default for LogisticRegression\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print accuracy and coefficients\n",
        "y_pred = model.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(\"L2 Regularization Accuracy:\", accuracy)\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "yWpFrji-Qij8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')C\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Apply Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga')  # Requires 'saga' solver\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(\"Elastic Net Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "1IBw6635QimP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "#multi_class='ovr'C\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris  # Using Iris dataset for multiclass\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Apply Logistic Regression with OvR\n",
        "model = LogisticRegression(multi_class='ovr')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(\"Multiclass (OvR) Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "hBVmTwWKQioi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "#Regression. Print the best parameters and accuracyC\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)  # cv=5 for 5-fold CV\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "VqZeIdbDQisB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "#average accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Apply Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5 splits\n",
        "accuracies = []\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average Accuracy (Stratified K-Fold):\", np.mean(accuracies))"
      ],
      "metadata": {
        "id": "KHc0_OWeQvDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "#accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data from CSV (replace 'your_file.csv' with your file path)\n",
        "data = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.iloc[:, :-1].values  # Features\n",
        "y = data.iloc[:, -1].values   # Target\n",
        "\n",
        "# Split, train, evaluate (same as in #1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (from CSV):\", accuracy)"
      ],
      "metadata": {
        "id": "G_3UWBZwQu_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "#Logistic Regression. Print the best parameters and accuracy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Define the parameter distribution (using distributions for RandomizedSearchCV)\n",
        "param_distributions = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Logarithmic scale for C\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    LogisticRegression(max_iter=1000),  # Increase max_iter for convergence\n",
        "    param_distributions,\n",
        "    n_iter=10,  # Number of random combinations to try\n",
        "    cv=5,\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    random_state=42\n",
        ")\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "dJzN-2YNQu81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracyM\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Apply One-vs-One Multiclass Strategy\n",
        "ovo = OneVsOneClassifier(LogisticRegression())  # You can specify the LogisticRegression parameters here\n",
        "ovo.fit(X, y)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = ovo.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(\"One-vs-One Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "lAFmppHEQu6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "#classificationM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(n_classes=2, random_state=0)  # Binary classification\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate and display the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])  # Assuming binary classes 0 and 1\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u6-DkxTORRrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "#Recall, and F1-Score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "id": "jjxFKZvyRRoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "#improve model performance\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.datasets import make_imbalance  # For creating imbalanced data\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_imbalance(n_samples=100, n_features=2,\n",
        "                      imb_ratio=0.1, random_state=42)  # 10:1 imbalance ratio\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with class weights\n",
        "model = LogisticRegression(class_weight='balanced')  # Or provide a dictionary for custom weights\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (with Class Weights):\", accuracy)"
      ],
      "metadata": {
        "id": "4JunEIN8RRmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "#evaluate performanceM\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Titanic dataset (replace 'titanic.csv' with your file path)\n",
        "titanic = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Select features and target\n",
        "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic['Survived']\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent'\n",
        "X['Age'] = imputer.fit_transform(X[['Age']])\n",
        "X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill with mode\n",
        "\n",
        "# Convert categorical features to numerical using one-hot encoding\n",
        "X = pd.get_dummies(X, columns=['Sex', 'Embarked'])\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (Titanic Dataset):\", accuracy)"
      ],
      "metadata": {
        "id": "xkOFfTvBRRkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "#model. Evaluate its accuracy and compare results with and without scalingM\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model (with scaling)\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy (with scaling)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Train Logistic Regression model (without scaling)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_"
      ],
      "metadata": {
        "id": "OwFf7HojRRiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "g_dSTYRnRRey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "#accuracy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom C\n",
        "model = LogisticRegression(C=0.5)  # Set C to 0.5\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (C=0.5):\", accuracy)"
      ],
      "metadata": {
        "id": "SgYiVcN2RRcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "#coefficients\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(n_features=5, random_state=0)  # Make sure you have feature names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get coefficients and feature names (assuming you have feature names)\n",
        "coefficients = model.coef_[0]\n",
        "feature_names = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5']  # Replace with your actual names\n",
        "\n",
        "# Print feature importances\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef}\")"
      ],
      "metadata": {
        "id": "Sp2SfiEMRRZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "#Score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa)"
      ],
      "metadata": {
        "id": "MgXoYIs4RRW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "#classification.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(n_classes=2, random_state=0)  # Binary classification\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate AUC-PR\n",
        "auc_pr = auc(recall, precision)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plt.plot(recall, precision, label=f'AUC = {auc_pr:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "27IhIbbJTNu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "#their accuracy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Solvers to try\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Train and evaluate with each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with {solver}: {accuracy}\")"
      ],
      "metadata": {
        "id": "BEc1yX59TNsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "#Correlation Coefficient (MCC)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
      ],
      "metadata": {
        "id": "5G212O2sTNp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "#accuracy to see the impact of feature scaling.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate on raw data\n",
        "model_raw = LogisticRegression()\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Train and evaluate on standardized data\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled."
      ],
      "metadata": {
        "id": "jK6eRNUzTNnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "#cross-validation\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Define the range of C values to try\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}  # Try values from 10^-4 to 10^4\n",
        "\n",
        "# Use GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(solver='liblinear'),  # Use a solver that supports L1/L2\n",
        "    param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5),  # Use StratifiedKFold for classification\n",
        "    scoring='accuracy',  # Optimize for accuracy\n",
        "    n_jobs=-1  # Use all available cores\n",
        ")\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best C value and corresponding accuracy\n",
        "print(\"Best C:\", grid_search.best_params_['C'])\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Train the final model with the best C value\n",
        "best_C = grid_search.best_params_['C']\n",
        "final_model = LogisticRegression(C=best_C, solver='liblinear')\n",
        "final_model.fit(X, y)"
      ],
      "metadata": {
        "id": "j57JB057TZRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "#make predictions.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from joblib import dump, load  # Import joblib for saving and loading\n",
        "\n",
        "# Create a sample dataset (replace with your data)\n",
        "X, y = make_classification(random_state=0)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file\n",
        "dump(model, 'logistic_regression_model.joblib')  # Save as 'logistic_regression_model.joblib'\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load('logistic_regression_model.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (Loaded Model):\", accuracy)"
      ],
      "metadata": {
        "id": "Mcb7SO5KTZOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkcEWjwZTZLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}